@article{LI2025104258,
title = {Provoking critical thinking: Using counter-arguments in online discussion summarisation},
journal = {Information Processing & Management},
volume = {62},
number = {6},
pages = {104258},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104258},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001992},
author = {Shangqian Li and Lei Han and Gianluca Demartini},
keywords = {Generative artificial intelligence, Online opinion modelling, Human-computer interaction, Human-centred computing},
abstract = {Generative AI systems based on Large Language Models (LLMs), like ChatGPT, have brought profound convenience to users thanks to their ability to summarise existing documents and to generate new text. This shows the potential to summarise online human discussions or debates for new entrants to quickly comprehend the ongoing matters and arguments and to get efficiently involved in the opinion deliberation process. However, generative AI has frequently been associated with negatively affecting users’ decision making. In this paper, we study a novel approach based on generative AI to trigger users’ critical thinking by challenging fresh counter-arguments after summarising existing online discussions for incoming users. We conduct a user study with 558 participants to determine the effectiveness and fairness of AI summarisation across three online platforms — Reddit, Kialo, and Debatewise. Our results show that the intervention methods and platform differences are strongly associated with participants’ level of opinion change and the strength of their belief. We found that participants’ opinion changes affected their perceived usefulness of the AI system. Our work opens the door to LLM applications helping Web users participate in online opinion deliberation more efficiently, with a higher level of critical thinking, and with a reduced negative attitude.}
}
@article{HWANG2025104266,
title = {Facilitating students’ critical thinking, metacognition and problem-solving tendencies in geriatric nursing class: A mixed-method study},
journal = {Nurse Education in Practice},
volume = {83},
pages = {104266},
year = {2025},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2025.104266},
url = {https://www.sciencedirect.com/science/article/pii/S1471595325000228},
author = {Gwo-Jen Hwang and Pei-Yu Cheng and Ching-Yi Chang},
keywords = {Generative AI-guided prompt-based learning, Critical thinking tendency, Metacognition tendency, Problem-solving tendency, Question generating ability},
abstract = {Aim
The aim of this study was to explore the use of generative artificial intelligence (GenAI) in geriatric nursing classes for the design of older adult activities to educate students on how to pose clear questions, provide and identify potentially suitable daily activities for older adults.
Background
Researchers in various educational fields are increasingly employing GenAI tools such as ChatGPT for curriculum development and research. Question generation is an essential skill for all students to learn to acquire knowledge. However, there is limited experimental evidence on teaching students to correctly use GenAI for assisting with question generation ability and empirical data related to improving students' capacity for solving complex problems remains scarce.
Design
A mixed-method study design with both quantitative and qualitative analysis.
Methods
This study investigated the effectiveness of a GenAI-guided prompt-based learning approach implemented in a geriatric nursing class for first-year undergraduate students, involving a cohort of 56 participants.
Results
Experimental results indicated that the GenAI-guided prompt-based learning approach significantly enhanced students' critical thinking, metacognition and problem-solving tendencies and their question generation via prompts performance. Moreover, participants who engaged in the GenAI-guided prompt-based learning approach found the tasks easier to complete and required less cognitive effort.
Conclusions
Nursing students using the GenAI-guided prompt-based learning approach outperformed the control group in cognitive network analysis dimensions of clarity, relevance, complexity, precision and engagement. Thus, integrating GenAI prompts into course activities can effectively improve student learning outcomes, reduce metacognitive load and assist in solving learning problems.}
}
@article{WANG2024103861,
title = {Conversational AI chatbots as counselors for hospitality employees},
journal = {International Journal of Hospitality Management},
volume = {122},
pages = {103861},
year = {2024},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2024.103861},
url = {https://www.sciencedirect.com/science/article/pii/S0278431924001737},
author = {Yao-Chin Wang and Oscar Hengxuan Chi and Hiroaki Saito and Yue (Darcy) Lu},
keywords = {Conversational AI, Chatbot, Generative AI, Artificial intelligence, Expectancy theory, Hospitality employee, Counseling, General app fatigue},
abstract = {Considering the advancement of generative artificial intelligence and the mental health challenges faced by hospitality employees, this study proposes and tests a scenario of counseling hospitality employees using conversational AI chatbots in a mobile app. Survey data were collected from 553 hospitality employees in Japan. Findings reveal that employees’ expected psychological safety of AI chatbots can be improved by AI chatbots’ cuteness of appearance and emotional capability, and their expected service quality in using AI chatbots can be enhanced by cuteness of appearance, emotional capability, and psychological safety. The service quality of AI chatbots then causes their use intention, which then strengthens their willingness to recommend and help colleagues’ usage of AI chatbots for counseling. Moreover, employees’ general app fatigue can moderate the effects of AI chatbots’ cuteness of appearance on psychological safety and service quality. This study offers valuable practical implications for developing conversational AI chatbots for hospitality employees.}
}
@article{CHEN2024,
title = {Semantic-Driven Paradigm Shift in Campus Guide Design Leveraging the KE-AIGC Framework},
journal = {International Journal on Semantic Web and Information Systems},
volume = {21},
number = {1},
year = {2024},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.368039},
url = {https://www.sciencedirect.com/science/article/pii/S1552628325000183},
author = {Xiaoqing Chen and Juanfen Wang and Yi Zhuang},
keywords = {Kansei Engineering, Generative Artificial Intelligence (AIGC), Guide Systems, University Campus, Semantic Web, User Experience},
abstract = {ABSTRACT
Campus guide systems are crucial to university infrastructure, shaping the experiences of students, staff, and visitors. Current systems face critical challenges in three areas: capturing diverse user needs, translating emotional requirements into design elements, and integrating campus cultural identity. This study integrates Kansei Engineering (KE) and Generative Artificial Intelligence (AIGC) to propose a semantic-driven design method. Using Semantic Web and Natural Language Processing (NLP), it models demand semantics, extracts emotional semantics such as safety and belonging, and maps them to design semantics for AIGC to generate personalized guide solutions. The approach leverages data-driven emotional semantic analysis and generative models to improve path guidance precision and cultural representation. Results indicate significant improvements in user experience, pathfinding accuracy, and cultural communication, with higher user satisfaction. This method provides a new semantic-driven pathway for developing campus guide systems and development prospects.}
}
@article{LIBERA2025,
title = {ChatGPT in the Working World:},
journal = {Information Resources Management Journal},
volume = {38},
number = {1},
year = {2025},
issn = {1040-1628},
doi = {https://doi.org/10.4018/IRMJ.386593},
url = {https://www.sciencedirect.com/science/article/pii/S1040162825000187},
author = {Pedro Schötteler Libera and Volker Bilgram and Sebastian Schötteler and Jan Mammen},
keywords = {Generative AI, Technology Adoption, Human-AI Interaction, Organizational Malleability, Qualitative Research, ChatGPT, Job Displacement, Future Work},
abstract = {ABSTRACT
The authors investigated how ChatGPT transforms workplace tasks by analyzing qualitative survey responses from 78 U.S. professionals in the fields of software, marketing, and academia. This study addresses a gap in understanding the malleability of generative artificial intelligence in diverse professional contexts, a need underscored by ChatGPT’s rapid adoption and mixed impact on work practices. Using a qualitative survey deployed via a validated platform, the authors collected open-ended responses about tasks, challenges, and opportunities. Responses were inductively coded to compare domain-specific applications. The findings show that although ChatGPT is widely used across sectors for tasks such as content creation, research, and idea generation, certain tasks—such as coding in software, strategic communication in marketing, and knowledge acquisition in academia—diverge. The results emphasize the importance of context-sensitive integration strategies and bottom-up adoption approaches to maximize the benefits of artificial intelligence while mitigating risks.}
}
@article{RIDER202581,
title = {Evaluating large language model performance to support the diagnosis and management of patients with primary immune disorders},
journal = {Journal of Allergy and Clinical Immunology},
volume = {156},
number = {1},
pages = {81-87},
year = {2025},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2025.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0091674925001666},
author = {Nicholas L. Rider and Yingya Li and Aaron T. Chin and Daniel V. DiGiacomo and Cullen Dutmer and Jocelyn R. Farmer and Kirk Roberts and Guergana Savova and Mei-Sing Ong},
keywords = {Primary immune disorders, inborn errors of immunity, large language models, generative AI, health care AI, medical chatbot},
abstract = {Background
Generative artificial intelligence (GAI) is transforming health care in a variety of ways; however, the present utility of GAI for supporting clinicians who treat rare disease such as primary immune disorders (PIs) is not well studied. We evaluated the ability of 6 state-of-the-art large language models (LLMs) for providing clinical guidance about PIs.
Objective
To quantitatively and qualitatively measure the utility of current, open-source LLMs for diagnosing and providing helpful clinical decision support about PIs.
Methods
Five expert clinical immunologists each provided 5 real-world, anonymized PI case vignettes via multi-turn prompting to 6 LLMs (OpenAI GPT-4o, Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, Mistral-7B-Instruct-v0.3, Mistral-Large-Instruct-2407, Mixtral-8x7B-Instruct-v0.1). We assessed the diagnostic accuracy of the LLMs and the quality of clinical reasoning using the Revised-IDEA (R-IDEA) score. Qualitative LLM assessment was made by immunologist narratives.
Results
Performance accuracy (>88%) and R-IDEA scores (≥8) were superior for 3 models (GPT-4o, Llama-3.1-70B-Instruct, Mistral-Large-Instruct-2407), with GPT-4o achieving the highest diagnostic accuracy (96.2%). Conversely, the remaining 3 models fell below acceptable accuracy rates near 60% or lower and had poor R-IDEA scores (≤0.55), with Mistral-7B-Instruct-v0.3 attaining the worst diagnostic accuracy (42.3%). Compared with the 3 best-performing LLMs, the 3 worst-performing LLMs had a substantially lower median R-IDEA score (P < .001). Interclass correlation coefficient for R-IDEA score assignments varied substantially by LLM, ranging from good to poor agreement, and did not appear to correlate with either diagnostic accuracy or median R-IDEA score. Qualitatively, immunologists identified several themes (eg, correctness, differential diagnosis appropriateness, relative conciseness of explanations) of relevance to PIs.
Conclusions
LLM can support diagnosis and management of PIs; however, further tuning is needed to optimize LLMs for best practice recommendations.}
}
@article{KAGIYAMA2025,
title = {PRIME 2.0: Proposed Requirements for Cardiovascular Imaging-Related Multimodal-AI Evaluation: An Updated Checklist},
journal = {JACC: Cardiovascular Imaging},
year = {2025},
issn = {1936-878X},
doi = {https://doi.org/10.1016/j.jcmg.2025.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X25004668},
author = {Nobuyuki Kagiyama and Márton Tokodi and Quincy A. Hathaway and Rima Arnaout and Rhodri Davies and Damini Dey and Nicolas Duchateau and Alan G. Fraser and Shinichi Goto and Ankush D. Jamthikar and Carolyn S.P. Lam and Evangelos K. Oikonomou and David Ouyang and Ambarish Pandey and Timothy J. Poterucha and Zahra Raisi-Estabragh and Jordan B. Strom and Qiang Zhang and Naveena Yanamala and Partho P. Sengupta},
keywords = {artificial intelligence, cardiovascular imaging, clinical validation, deep learning, large language models, model development, multimodal generative artificial intelligence, PRIME 2.0 checklist, transparency and reproducibility},
abstract = {The PRIME (Proposed Requirements for Cardiovascular Imaging–Related Machine Learning Evaluation) 2.0 checklist is an updated, domain-specific framework designed to standardize the development, evaluation, and reporting of artificial intelligence (AI) applications in cardiovascular imaging. This update specifically responds to rapid advances from traditional machine learning to deep learning, large language models, and multimodal generative AI. The updated checklist was developed through a modified Delphi process by an international panel of clinical and technical experts. In contrast to general AI reporting guidelines, it delivers detailed, practical recommendations on all critical aspects of AI research and builds upon the original 7-domain framework by incorporating cardiovascular imaging–specific complexities such as cardiac motion, imaging artifacts, and interobserver variability. By promoting transparency and rigor, PRIME 2.0 can serve as a vital resource for researchers, clinicians, peer reviewers, and journal editors working at the forefront of AI in cardiovascular imaging.}
}
@article{LI2025100445,
title = {Two years of innovation: A systematic review of empirical generative AI research in language learning and teaching},
journal = {Computers and Education: Artificial Intelligence},
volume = {9},
pages = {100445},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100445},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000852},
author = {Belle Li and Yaling Lily Tan and Chaoran Wang and Victoria Lowell},
keywords = {Generative artificial intelligence, Language education, Systematic review, ChatGPT, Empirical research, Language learning and teaching},
abstract = {This systematic review examines the evolution of empirical research on generative AI in language learning and teaching from 2023 to 2024. Following PRISMA guidelines, we analyzed 144 peer-reviewed articles from Web of Science, Scopus, and ERIC databases to explore the field's progression, including identifying new developments, shifts in research priorities, and emerging themes that have arisen compared to the first year. The findings reveal an exponential growth in publications, with a significant shift from primarily exploratory inquiries to more systematic and empirically driven investigations. The analysis identified six main research foci: Perceptions and attitudes, psychological and cognitive aspects, teaching and learning strategies, language skills development, writing and feedback, and implementation and integration. While higher education (86.7 %) and English as Foreign Language contexts (86.1 %) dominated the research landscape, there was notable geographical diversity, with strong representation from East Asia and emerging contributions from the Middle East. Mixed-methods approaches (38.9 %) were prevalent, with increasing incorporation of AI-generated content, multimedia recordings, and digital interaction data from 2023 to 2024. Writing emerged as the primary focus (42.4 %) while speaking, listening, and reading skills received comparatively less attention. This review highlights critical gaps, including limited research in K-12 settings, insufficient longitudinal studies, and the need for more diverse language representation beyond English. These findings suggest the field is maturing but requires broader investigation across educational levels, language domains, and geographical contexts to fully understand the impact of generative AI in language education.}
}
@article{XIAO2024104125,
title = {A fair and scalable watermarking scheme for the digital content trading industry},
journal = {Computers in Industry},
volume = {161},
pages = {104125},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104125},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000538},
author = {Xiangli Xiao and Moting Su and Jiajia Jiang and Yushu Zhang and Zhongyun Hua and Zhihua Xia},
keywords = {Digital content trading, buyer–seller watermarking, Cryptography, Client-side embedding},
abstract = {The booming Internet economy and generative artificial intelligence have driven the rapid growth of the digital content trading industry, creating an urgent need for the fair protection of the rights of both buyers and sellers. To meet this need, a technique known as buyer–seller watermarking has emerged. Despite its existence, the majority of existing buyer–seller watermarking schemes adopt the owner-side embedding mode, which results in poor scalability. While a handful of schemes adopt the client-side embedding mode to enhance scalability, they either require the deep involvement of a trusted third party or fall short of ensuring complete fairness due to the unresolved unbinding problem. To address these challenges, this paper proposes a fair and scalable watermarking scheme for digital content transactions based on proxy re-encryption and digital signatures. For one thing, this scheme solves the unbinding problem and ensures complete fair protection of the rights of both buyers and sellers. For another, it adopts the client-side embedding mode and has good scalability. Additionally, it eliminates the need for a trusted third party. Finally, theoretical analysis and experiments demonstrate that the proposed scheme achieves the intended design goals and possesses superior efficiency advantages.}
}
@article{YUSUF2024101619,
title = {Implementing a proposed framework for enhancing critical thinking skills in synthesizing AI-generated texts},
journal = {Thinking Skills and Creativity},
volume = {53},
pages = {101619},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101619},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124001573},
author = {Abdullahi Yusuf and Shamsudeen Bello and Nasrin Pervin and Abdullahi Kadage Tukur},
keywords = {Critical thinking, AI-generated texts, Framework, Epistemic network analysis, Co-occurrences},
abstract = {Since the development of open and low-cost generative artificial intelligence (GenAI), the higher education community has witnessed high use of AI-generated texts in scholarly research and academic assignments, attracting ongoing debate about whether such practices constitute cheating. While scholars argue that integrating GenAI tools can enhance productivity, critics raise concerns about the negative effect of such integration on critical thinking (CrT). This study therefore proposed a framework for enhancing students’ CrT skills in synthesizing AI-generated information. The proposed framework is underpinned by various theoretical foundations, encompassing five interconnected step-wise phases (familiarizing, conceptualizing, inquiring, evaluating, and synthesizing). The study was conducted under two separate experiments. The first experiment (Study 1) validated the effectiveness of the proposed framework, providing CrT training to 179 postgraduate students. In the second study (n = 125), additional experiments were undertaken to confirm the effectiveness of the framework in different contexts. An experimental procedure involving pretest and posttest design was implemented wherein participants were randomly allocated to one of three groups: experimental group 1 (exposed to our framework), experimental group 2 (exposed to an alternative self-regulated learning framework), and a control group (exposed to a non-structured framework). Results from Study 1 revealed that the framework enhances students’ CrT skills to synthesize AI-generated texts. However, these CrT skills manifested through various rigorous training aimed at reinforcing learning. While the proposed framework holds considerable value in cultivating CrT skills, significant differences arise across various personality traits. In Study 2, the framework proved to be effective in different contexts. However, it did not make a difference, particularly in its capacity to enhance students’ self-regulated learning compared to other frameworks. We discussed the implications of the findings and recommended it to educators seeking to prepare students for the challenges of the AI-driven knowledge economy.}
}
@article{HUANG2025104152,
title = {Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104152},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104152},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000937},
author = {Yinghui Huang and Weijun Wang and Jinyi Zhou and Liang Zhang and Jionghao Lin and Hui Liu and Xiangen Hu and Zongkui Zhou and Wanghao Dong},
keywords = {ChatGPT, Mental health Q&A, Large language model, Prompt engineering, Integrative modeling, LLMs evaluation},
abstract = {Recent advancements in generative artificial intelligence (GenAI), particularly ChatGPT, have demonstrated significant potential in addressing the persistent treatment gap in mental health care. Systematic evaluation of ChatGPT’s capabilities in addressing mental health questions is essential for its large-scale application. The current study introduces a computational evaluation framework centered on perceived information quality (PIQ) to quantitatively assess ChatGPT’s capabilities. Leveraging datasets of question-answer pairs generated by both humans and ChatGPT, the framework integrates predictive modeling, explainable modeling, and prompt-engineering-based validation to identify intrinsic evaluation metrics and enable automated assessments. Results revealed that unprompted ChatGPT’s PIQ is significantly lower than that of human counselors overall, with notable deficiencies such as insufficient conversational length, lower text diversity, and reduced professionalism. Despite not matching the top 25% of human counselors, our evaluation framework improved ChatGPT’s mean PIQ by 8.91% to 11.67% across four risk levels. Prompted ChatGPT performed comparably to human counselors in severe (p = 0.0561) and moderate-risk questions (p = 0.7851), and significantly outperformed them in low- and no-risk categories by 6.80% and 4.63%, respectively (p < 0.001). However, undesirable verbal behaviors still persist in text diversity and professionalism. These findings validate ChatGPT’s capabilities to address mental health questions while cautioning that further researches are necessary for LLM-based mental health systems to deliver services comparable to human experts.}
}
@article{GUO2024105787,
title = {Automatic assessment of concrete cracks in low-light, overexposed, and blurred images restored using a generative AI approach},
journal = {Automation in Construction},
volume = {168},
pages = {105787},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105787},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524005235},
author = {Pengwei Guo and Xiangjun Meng and Weina Meng and Yi Bao},
keywords = {Blurred image, Computer vision, Conditional Generative Adversarial Network (CGAN), Crack inspection, Deep learning, Generative AI, Image restoration, Low-light image, Overexposed image},
abstract = {Deep learning-based computer vision techniques have high efficiency in assessing concrete cracks from images, and the assessment can be automated using robots for higher efficiency. However, assessment accuracy is often compromised by low-quality images. This paper presents a Conditional Generative Adversarial Network (CGAN)-based approach to restore low-light, overexposed, and blurred images. The approach integrates attention mechanisms and residual learning and uses Wasserstein loss with gradient penalty. Crack assessment results show that the proposed approach outperforms state-of-the-art methods, regarding structural similarity (SSIM: 0.78 for deblurring, 0.95 for low-light enhancement, and 0.96 for overexposure correction) and peak signal-to-noise ratio (PSNR: 28.6 for deblurring, 31.4 for low-light enhancement, and 31.6 for overexposure correction). Restored images have been used to train a deep learning model for assessing concrete cracks. The Intersection over Union (IoU) and F1 score of crack segmentation are higher than 0.98 and 0.99, respectively, revealing high accuracy in crack assessment tasks.}
}
@article{MOUSAVI2025125296,
title = {Revolutionizing solar energy resources: The central role of generative AI in elevating system sustainability and efficiency},
journal = {Applied Energy},
volume = {382},
pages = {125296},
year = {2025},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2025.125296},
url = {https://www.sciencedirect.com/science/article/pii/S0306261925000261},
author = {Rashin Mousavi and Arash Mousavi and Yashar Mousavi and Mahsa Tavasoli and Aliasghar Arab and Ibrahim Beklan Kucukdemiral and Alireza Alfi and Afef Fekih},
keywords = {Generative artificial intelligence, Solar energy systems, AI-driven solar solutions, Solar photovoltaic systems design and optimization, Solar systems predictive maintenance},
abstract = {Driven by growing environmental concerns, such as global warming and the depletion of fossil fuels, the renewable energy industry, particularly solar energy, has risen to global prominence. In this context, generative artificial intelligence (Gen-AI) can play a valuable role in facilitating the development of more efficient, durable, and adaptable solar systems. Gen-AI’s multifaceted proficiency, from predictive maintenance and reducing downtime and costs to vital forecasting for grid management and strategic planning, extends to optimizing site selection for solar farms and smart grid integration, thereby enhancing solar energy flow, grid stability, and sustainable operation. This paper presents a comprehensive exploration of the role of Gen-AI in revolutionizing the solar energy industry. Focusing on various aspects of solar energy systems, including design, optimization, sizing, maintenance, energy forecasting, site selection, and smart grid integration, the study investigates the transformative impact of Gen-AI across these domains. It demonstrates how Gen-AI enhances the efficiency, sustainability, and adaptability of solar systems, driving strategic decision-making and optimizing the integration of solar power within complex energy ecosystems. Furthermore, the paper concludes by discussing the challenges and future prospects of employing Gen-AI in the solar energy domain, providing a comparative analysis of the current and future scenarios, and underscoring the advantages, disadvantages, and challenges of Gen-AI implementation.}
}
@article{UKWANDU2025103616,
title = {The future of teaching and learning in the context of emerging artificial intelligence technologies},
journal = {Futures},
volume = {171},
pages = {103616},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2025.103616},
url = {https://www.sciencedirect.com/science/article/pii/S0016328725000783},
author = {Elochukwu Ukwandu and Omobolanle Omisade and Karl Jones and Simon Thorne and Mike Castle},
keywords = {Generative artificial intelligence, Prompt technologies, Artificial intelligence, ChatGPT, AI-Agents, Future of teaching and learning, Emerging AI disruptive technologies},
abstract = {In the context of emerging artificial intelligence technologies (AI) such as AI-Bots (ChatGPT) and AI-Agents, it is imperative that adequate adjustment be made, and also seen to be made. However, this has to be done from an informed positions. There is no doubt that these disruptive technologies are changing the way we live, conduct our day-to-day businesses, teach, learn and conduct research. There are also emerging concerns that these dynamics may result in a paradigm shift from student-teacher relationship to student-AI-Tutor-based relationship within the academic circle. Besides, there are foreseeable dangers of compromising academic integrity through high-technology plagiarism and the potentials of students avoiding learning through AI deployment and utilisation in their academic pursuits. But something worth considering is how applying these tools in education will potentially change the entire classroom experience of students, their knowledge and skills outcomes that are relevant in this AI era. This position paper is an effort to put into context what the authors of this paper forecast as the future of teaching and learning in the context of these inevitable disruptions to education activities and its subsectors as we currently know it. The authors found it necessary to take these positions to help bring to fore some practical use cases of AI in education; recent developments and theoretical frameworks in literature, technical reports, as well as experts opinions that can help assuage stakeholder’s concerns despite some obvious existing challenges. It is our view that this paper will be found useful by educators, stakeholders and administrators in the areas of curriculum design, classroom administration and entire academic planning and reviews.}
}
@article{KRAKOWSKI2025100560,
title = {Human-AI agency in the age of generative AI},
journal = {Information and Organization},
volume = {35},
number = {1},
pages = {100560},
year = {2025},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2025.100560},
url = {https://www.sciencedirect.com/science/article/pii/S1471772725000065},
author = {Sebastian Krakowski},
keywords = {Generative artificial intelligence, Automation, Augmentation, Human-AI agency, Machine learning, Innovation management},
abstract = {The rapid emergence of generative artificial intelligence (GenAI) is profoundly transforming the nature of work and organizations, challenging prevalent views of AI as primarily enabling prediction and optimization. This paper argues that GenAI represents a qualitative shift that necessitates a fundamental reassessment of AI's role in management and organizations. By identifying and analyzing four critical dimensions (i) GenAI's broad applicability as a general-purpose technology; (ii) its ability to catalyze exploratory and combinatorial innovation; (iii) its capacity to enhance cognitive diversity and decision-making; and (iv) its democratizing effect on AI adoption and value creation the paper highlights GenAI's potential to augment and scale human creativity, learning, and innovation. Building on insights from the AI and management literature, as well as on theory of human-AI agency, the paper develops a novel perspective that challenges the dominant efficiency-oriented narrative. It proposes that a human-complementary approach to GenAI development and implementation, leveraging it as a generative catalyst for exploration, can enable radically increased creativity, innovation, and growth. GenAI's democratizing aspects can amplify these mechanisms, promoting widely shared growth when combined with appropriate policy and managerial choices. Implications for theory, practice, and future research directions are discussed, drawing attention to the need for approaches in GenAI development and deployment that are complementary rather than competitive to human beings. The paper concludes by discussing the theoretical, practical, and policy implications of this transformative technology. It outlines future research directions, emphasizing the critical role of human agency in determining the organizational, societal, and ethical outcomes associated with AI adoption and implementation.}
}
@article{RADAIDEH2025101287,
title = {A Bayesian ensemble approach for improved sustainable aviation fuel modeling},
journal = {Energy Conversion and Management: X},
pages = {101287},
year = {2025},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2025.101287},
url = {https://www.sciencedirect.com/science/article/pii/S2590174525004192},
author = {Mohammed I. Radaideh and Majdi I. Radaideh and Angela Violi},
keywords = {Sustainable aviation fuels, Bayesian regression, Bayesian neural networks, Uncertainty quantification, Ensemble mixing rules},
abstract = {In this work, we introduce a new methodology to combine the available methods to predict the properties of complex hydrocarbon mixtures such as aviation fuels. Due to the complexity of aviation fuels, the available methods perform well individually on some of the experimental observations and vice versa on others when a surrogate aviation fuel is defined and used. To this end, we introduce a new ensemble model based on the existing methods that combine and weigh their predictions. We employ the probabilistic Bayesian approach to predict aviation fuel properties with confidence levels. This is necessary because the available experimental data for aviation fuels is generally limited, which leads to overfitting. We adopt both “interpretable” Bayesian regression and a more “black-box” approach to Bayesian neural networks. An ensemble of predictive methods provided better predictions than the individual methods with robust confidence levels for three properties considered: mass density, kinematic viscosity, and flash point. A significant reduction in the mean absolute percentage error was obtained for mass density predictions, from 1.25% to 0.57% and 0.42%, using the Bayesian linear regression (BLR) and Bayesian Neural Network (BNN), respectively. The error in kinematic viscosity predictions was reduced from 17.25% to 9.02% and 6.79% using BLR and BNN, respectively. The error in flash point predictions is reduced from 9.04% to 5.83% by BLR and to 5.51% by BNN. The importance of the methods in the ensemble did not fully follow their individual performance, where the accurate models may not be the most important. The ensemble approach allows for the inclusion of new methods, even if they are slightly less accurate. This methodology can be extended to predict other aviation fuel properties and incorporate any predictive model. It also offers a way to generate valid training data for generative Artificial Intelligence (AI) models, helping to address the scarcity of aviation fuel data.}
}
@article{KHAN2025106506,
title = {Generative AI approaches for architectural design automation},
journal = {Automation in Construction},
volume = {180},
pages = {106506},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106506},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525005461},
author = {Adeer Khan and Seongju Chang and Hojong Chang},
keywords = {Floorplans, Automation, Generative design, Artificial intelligence, Automated building design, Architecture, Layout optimization},
abstract = {This review examines the potential and challenges of Generative Artificial Intelligence (AI) in automated building design within architectural practice. A comprehensive analysis of advanced generative models is conducted to evaluate their performance across eight architectural criteria. The qualitative assessment indicates that hybrid approaches combining diffusion models with autoregressive techniques provide the most promising outcomes for architectural applications. Despite advancements, significant challenges remain, including scalability limitations, fragmented workflow integration, and the lack of standardized evaluation frameworks. Potential solutions are identified through interdisciplinary collaboration and strategic research directions, such as developing unified evaluation metrics, enhancing model adaptability, integrating energy-optimized design generation for sustainability, and incorporating designer input in AI-driven workflows. This review provides a structured evaluation of current generative design approaches while proposing a roadmap for future research that bridges the gap between AI innovation and practical architectural implementation, ultimately advancing the field toward more efficient, creative, and sustainable building design automation.}
}
@article{WISS2025100752,
title = {Development of the AI Acceptance Scale for Interprofessional Education (AAIPE) and Collaborative Practice Settings},
journal = {Journal of Interprofessional Education & Practice},
volume = {40},
pages = {100752},
year = {2025},
issn = {2405-4526},
doi = {https://doi.org/10.1016/j.xjep.2025.100752},
url = {https://www.sciencedirect.com/science/article/pii/S2405452625000151},
author = {Andrew Wiss and Dawn Joosten-Hagye and Jennifer Pattershall-Geide and Mary Showstark and Elke Zschaebitz and Kirsten Potter and Erin Embry and Heather Hageman and Patti Brooks},
keywords = {AI acceptance, Measurement of AI acceptance, Interprofessional education settings, Interprofessional collaboration and AI, Validated scale, Assessment, Generative artificial intelligence, GenAI and healthcare settings},
abstract = {Background
As artificial intelligence (AI) based tools become a more prevalent part of the work taking place in health and healthcare settings, students preparing for health profession roles will be asked with increasing frequency to adopt and integrate these tools into their developing knowledge and skills-sets. Because of this, developing an understanding of levels of AI acceptance, and the factors that play into that acceptance will be essential for supporting individuals training for health workforce roles and their collaborative work within and across disciplines.
Purpose
This paper describes the methodology utilized to create and then validate the Artificial Intelligence Acceptance Scale for Interprofessional Education (AAIPE). This validated scale is intended to measure health sector student levels of acceptance of artificial intelligence as a part of their workplace roles and responsibilities.
Method
The AAIPE scale was utilized at the conclusion of multi-discipline interprofessional education activity (N = 161).
Results
Analysis of the AAIPE results indicated moderate-to-high levels of internal consistency for scale items. Student participant AAIPE scores indicated neutral-to-moderately positive levels of acceptance overall without significant difference between students from different health sector academic programs.
Conclusions
This research uncovered lower levels of student acceptance of artificial intelligence's influence on professional ethics and AI's influence on role clarity. Higher levels of acceptance relating to AI as an evolving component of health sector work were also found. A discussion of these results relating to interprofessional education and practice is conducted.}
}
@article{HUSSAIN2024100071,
title = {Exploring audience engagement with ChatGPT-related content on YouTube: Implications for content creators and AI tool developers},
journal = {Digital Business},
volume = {4},
number = {1},
pages = {100071},
year = {2024},
issn = {2666-9544},
doi = {https://doi.org/10.1016/j.digbus.2023.100071},
url = {https://www.sciencedirect.com/science/article/pii/S2666954423000194},
author = {Khalid Hussain and M. Laeeq Khan and Aqdas Malik},
keywords = {ChatGPT, YouTube, Social media, Artificial intelligence, AI, Content creation, Content consumption, Engagement},
abstract = {The emergence of ChatGPT in the broader field of generative artificial intelligence (AI) has sparked scholarly discourse on its utilization in various disciplines. Yet, a significant void exists in our understanding of the dynamics of consumer engagement with content creators producing ChatGPT-related content. Therefore, the present study aims to delineate how ChatGPT-related content garners consumer engagement on YouTube. Data from 100 YouTube videos amassing an aggregate of 65 million views on ChatGPT were extracted using three application programming interfaces (APIs), namely, VidIQ, Tubebuddy, and SocialBlade. We subsequently contrasted this dataset with data from 200 other videos produced by the same creators. The data were analyzed using one-way ANOVA, multigroup SEM, and comparative line graphs. Employing the Uses and Gratifications (U&G) theoretical framework, our results indicate that innovative content such as ChatGPT-related videos garners more engagement than other content types from the same YouTube channels. Intriguingly, this study finds that ChatGPT-focused content exhibited diminished sensitivity to channel subscriber counts, with channels having fewer subscribers achieving higher viewership numbers. Furthermore, ChatGPT-related content induced a surge in new subscribers to the channel compared to the other content types. The present study pioneers the investigation of audience engagement with ChatGPT-related content by juxtaposing it with other content from the same YouTube channels. We also explicate the relationship between content sensitivity and extant subscriber counts. The present study provides vital insights and implications for a diverse audience, including content creators, developers of AI tools, advertisers, and content publishers.}
}
@article{JOSE2024101082,
title = {Can generative AI motivate management students? The role of perceived value and information literacy},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101082},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101082},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001538},
author = {Emily Maria K Jose and Akshara Prasanna and Bijay Prasad Kushwaha and Madhumita Das},
keywords = {, , , , , },
abstract = {Generative Artificial Intelligence (GenAI) is a disruptive technology that has started to be used among students in management education. However, the question is whether the utilisation of GenAI in educational settings stimulates students to engage in learning activities and broaden their knowledge base. Hence, this study investigates student motivation and perception of using GenAI (ChatGPT) technology in management education. A random sampling technique was employed to survey 478 students from various educational institutions in the southern region of India. The outcomes revealed that GenAI presents both prospects and hurdles in the domain of management education. The disruptive nature of this technology brings forth numerous opportunities for acquiring knowledge and augmenting one's cognitive capacity. Nonetheless, a cautious and accountable approach is imperative for the successful integration of GenAI into the of management education. Consequently, this study provides pragmatic implications for students, educators, and educational institutions. The effectiveness of GenAI in practical settings can be heightened by arranging interactive training sessions led by AI experts, devising easily accessible online educational modules, embedding AI proficiencies into educators through collaborative endeavors or specialized training programs, and establishing systematic assessment protocols to ensure continual improvement.}
}
@article{WAQARAKRAM2025113777,
title = {Advancing photovoltaic cells defect detection in electroluminescence images through exploring multiple object detectors},
journal = {Solar Energy Materials and Solar Cells},
volume = {292},
pages = {113777},
year = {2025},
issn = {0927-0248},
doi = {https://doi.org/10.1016/j.solmat.2025.113777},
url = {https://www.sciencedirect.com/science/article/pii/S0927024825003782},
author = {M. {Waqar Akram} and Jianbo Bai and Chen Xuan and Xie Xiaotuo and Jiayu Hu and Shaojie Wu},
keywords = {Automatic defect detection, Photovoltaic cells, Electroluminescence images, Deep learning, Unknown imaging conditions, Object detectors},
abstract = {Automated methods can provide accurate, time-efficient and cost-effective solutions for monitoring of photovoltaic (PV) modules in order to deal with underperformance and unreliability issues. However, these methods are not yet widespread at commercial level and still under study at laboratory scale due to many practical limitations. The present study deals with deep learning based enhanced classification and detection of multi-defects in electroluminescence (EL) images of PV cells, with a focus on practical application in field particularly on unseen data in multiple unknown imaging conditions. It explores potential of multiple state-of-the-art deep learning object detectors i.e. Detection Transformer, EfficientDet, FasterRCNN, YOLOv7, YOLOv8, and YOLOv9 with different variants, formations, techniques and experimentation, aiming enhancement in defect detection and understandings into trade-offs. These detectors were trained on polycrystalline cells with cracks, finger interruptions, black cores, and thick line defects. The proposed YOLOv9 GELAN-e with PGI and GELAN architectures achieves promising results of 94.30 % mAP@0.5. Moreover, testing experimentation is carried out on unseen data obtained in unknown imaging conditions (taken from multiple sources like a PV inspection center and public sources) and having wider diversity to generalize findings for providing insights into real challenges at practical level and elucidating possible solutions/directions. The proposed network also performed well on this data, which includes half-cut and full poly/mono crystalline cell images as well as Generative (Artificial Intelligence generated) images, making it promising for a wider range of practical applications.}
}
@article{SMERDON2024100288,
title = {AI in essay-based assessment: Student adoption, usage, and performance},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100288},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100288},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000912},
author = {David Smerdon},
keywords = {Artificial intelligence, Assessment, Academic integrity, Educational technologies, Higher education},
abstract = {The rise of generative artificial intelligence (AI) has sparked debate in education about whether to ban AI tools for assessments. This study explores the adoption and impact of AI tools on an undergraduate research proposal assignment using a mixed-methods approach. From a sample of 187 students, 69 completed a survey, with 46 (67%) reporting the use of AI tools. AI-using students were significantly more likely to be higher-performing, with a pre-semester average GPA of 5.46 compared to 4.92 for non-users (7-point scale, p = .025). Most students used AI assistance for the highest-weighted components of the task, such as the research topic and methods section, using AI primarily for generating research ideas and gathering feedback. Regression analysis suggests that there was no statistically significant effect of AI use on student performance in the task, with the preferred regression specification estimating an effect size of less than 1 mark out of 100. The qualitative analysis identified six main themes of AI usage: idea generation, writing assistance, literature search, grammar checking, statistical analysis, and overall learning impact. These findings indicate that while AI tools are widely adopted, their impact on academic performance is neutral, suggesting a potential for integration into educational practices without compromising academic integrity.}
}
@article{HUANG2025103775,
title = {Can ChatGPT serve as a writing collaborator? Insights from Chinese EFL learners},
journal = {System},
volume = {133},
pages = {103775},
year = {2025},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2025.103775},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X2500185X},
author = {Yu Huang and Di Wang},
keywords = {Generative AI, L2 writing, Student-chatbot collaboration, English as a foreign language (EFL)},
abstract = {Although integrating generative artificial intelligence chatbots into L2 writing classrooms has grown in recent years, little is known about the nature of student-chatbot collaboration and its impact on students' final writing products. This study investigates how Chinese EFL learners interact with ChatGPT in an argumentative writing task and examines the influence of student-chatbot collaboration on the quality of writing, as well as students' perceptions of ChatGPT as a writing collaborator. Conducted at a tier-one university in China, this study involved nine students with over ten years of English learning experience and upper-intermediate English proficiency. Participants completed an argumentative writing task with ChatGPT, followed by semi-structured interviews. Data from interaction logs, think-aloud protocols, final writing texts, and interviews were qualitatively analysed. Findings revealed a “student-directed, chatbot-mediated” interaction mode, where students actively directed the writing process and used ChatGPT to enhance idea generation, develop arguments, provide evidence, and support language use. ChatGPT also contributed to lexical and syntactic variety, offering feedback on grammar and genre conventions. Students perceived ChatGPT as a helpful tool for inspiring ideas and improving language expression. Compared to peer collaboration, ChatGPT was seen as advantageous due to its advanced language proficiency, extensive knowledge base, and flexibility. However, some students reported challenges, such as difficulties in processing the vast amount of information provided by ChatGPT and frustrations when the chatbot did not fully understand their needs or provided unexpected responses. This study concludes with a discussion on the potential pedagogical implications of integrating chatbots like ChatGPT into L2 writing instruction.}
}
@article{LEE2025100221,
title = {Generative AI risks and resilience: How users adapt to hallucination and privacy challenges},
journal = {Telematics and Informatics Reports},
volume = {19},
pages = {100221},
year = {2025},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2025.100221},
url = {https://www.sciencedirect.com/science/article/pii/S2772503025000362},
author = {Chunsik Lee and Junga Kim and Joon Soo Lim and Donghee Shin},
keywords = {AI hallucinations, Privacy concerns, Adaptive behavior, Information verification, Privacy protection behavior, Continuance intention},
abstract = {Purpose
This study examines two central risks affecting continued use of generative AI (GenAI)—AI hallucinations and privacy concerns—and explores how protective behaviors serve as adaptive mechanisms to mitigate these risks.
Design/methodology/approach
Drawing on Protection Motivation Theory, the study tests a risk-adaptive GenAI use model using survey data from 789 users recruited via a Prolific panel. Structural equation modeling is employed to analyze direct and moderating effects.
Findings
Privacy concerns negatively influence user attitudes while positively predicting both personal and system-level protective behaviors. Hallucination risk is similarly associated with negative attitudes but positively predicts information verification. Notably, only information verification significantly moderates the link between attitude and continuance intention.
Originality
The study extends Protection Motivation Theory to the GenAI context by developing and validating new constructs—hallucination risk and system-level privacy-protective behaviors. It reveals how different types of risk trigger distinct behavioral adaptations, highlighting the dual role of risk as both a deterrent and catalyst in GenAI adoption.
Practical implications
The study offers insights that risk perceptions may not deter continued use of GenAI when users perform risk protective behaviors. By highlighting which protective behaviors enhance continued use, the findings inform risk-mitigation strategies for developers, educators, and regulators.}
}
@article{KARACAY2025e402,
title = {Nursing Students' Experiences Towards Using ChatGPT as a Teaching Tool: A Descriptive Qualitative Study},
journal = {Teaching and Learning in Nursing},
volume = {20},
number = {2},
pages = {e402-e407},
year = {2025},
issn = {1557-3087},
doi = {https://doi.org/10.1016/j.teln.2024.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1557308724002658},
author = {Pelin Karaçay and Özgen Yaşar},
keywords = {Artificial intelligence, ChatGPT, nursing education, nursing student, teaching tool},
abstract = {Background
Nowadays, nursing students’ use of generative artificial intelligence tools such as ChatGPT in education is inevitable. Therefore, nurse educators can guide students using ChatGPT as a teaching tool. Studies investigating ChatGPT as a teaching tool in nursing education are very limited in the literature.
Aim
This study aimed to reveal sophomore nursing students' experiences and perspectives toward ChatGPT activities used as a teaching tool in the classroom.
Methods
The study used a descriptive qualitative design and a researcher-developed survey. The study sample consisted of 24 sophomore nursing students who participated in ChatGPT activities in the classroom and agreed to participate in the survey. Data were analyzed by using descriptive thematic analysis.
Results
Classroom ChatGPT activities improved the participants' knowledge of the subject matter and their knowledge of the ChatGPT application. These activities also promoted critical thinking and increased awareness regarding the use of artificial intelligence tools among the participants.
Conclusion
In-class ChatGPT activities can enhance nursing students' knowledge of the subject and the ChatGPT application and provide them with a critical perspective.}
}
@article{CHEN2024,
title = {Generative AI in Medical Practice: In-Depth Exploration of Privacy and Security Challenges},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/53008},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124001055},
author = {Yan Chen and Pouyan Esmaeilzadeh},
keywords = {artificial intelligence, AI, generative artificial intelligence, generative AI, medical practices, potential benefits, security and privacy threats},
abstract = {As advances in artificial intelligence (AI) continue to transform and revolutionize the field of medicine, understanding the potential uses of generative AI in health care becomes increasingly important. Generative AI, including models such as generative adversarial networks and large language models, shows promise in transforming medical diagnostics, research, treatment planning, and patient care. However, these data-intensive systems pose new threats to protected health information. This Viewpoint paper aims to explore various categories of generative AI in health care, including medical diagnostics, drug discovery, virtual health assistants, medical research, and clinical decision support, while identifying security and privacy threats within each phase of the life cycle of such systems (ie, data collection, model development, and implementation phases). The objectives of this study were to analyze the current state of generative AI in health care, identify opportunities and privacy and security challenges posed by integrating these technologies into existing health care infrastructure, and propose strategies for mitigating security and privacy risks. This study highlights the importance of addressing the security and privacy threats associated with generative AI in health care to ensure the safe and effective use of these systems. The findings of this study can inform the development of future generative AI systems in health care and help health care organizations better understand the potential benefits and risks associated with these systems. By examining the use cases and benefits of generative AI across diverse domains within health care, this paper contributes to theoretical discussions surrounding AI ethics, security vulnerabilities, and data privacy regulations. In addition, this study provides practical insights for stakeholders looking to adopt generative AI solutions within their organizations.}
}
@article{HONG2025105241,
title = {AI-supported idea-developing discourse to foster professional agency within teacher communities for STEAM lesson design in knowledge building environment},
journal = {Computers & Education},
volume = {229},
pages = {105241},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105241},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000090},
author = {Huang-Yao Hong and Mei-Ju Chen and Chih-Hsuan Chang and Li-Ting Tseng and Ching Sing Chai},
keywords = {Interdisciplinary projects, Learning communities, Online learning, Professional agency, STEAM},
abstract = {This study employed design-based research, comprising two design cycles to empower teachers' agency when they are engaged in STEAM lesson design. Cycle-1 focused on the application of a principle of “sustained idea-developing discourse” (SIDD) in an online knowledge building environment to foster teachers' “professional agency within teacher communities” (PATC). In cycle-2, generative artificial intelligence (GAI) was additionally integrated into the SIDD processes within the teacher community. The findings in cycle-1 indicated a general lack of interdisciplinary knowledge among the teachers for effective STEAM lesson design. The online collaborative discourse mainly focused on shallow idea-sharing and the PATC was not significantly enhanced. Nevertheless, with GAI integrated in the SIDD process in cycle-2, participants exhibited more elaborated, synthesis-oriented discourse actions, with significant enhancements in PATC. Overall, these findings emphasize the significance of GAI support in fostering SIDD for empowering professional teacher agency during their STEAM lesson design. The findings offer educators a more profound understanding of how to constructively use GAI for professional development in teacher communities.}
}
@article{CLARK2025,
title = {The Ability of AI Therapy Bots to Set Limits With Distressed Adolescents: Simulation-Based Comparison Study},
journal = {JMIR Mental Health},
volume = {12},
year = {2025},
issn = {2368-7959},
doi = {https://doi.org/10.2196/78414},
url = {https://www.sciencedirect.com/science/article/pii/S2368795925000812},
author = {Andrew Clark},
keywords = {adolescent mental health, AI therapy, digital therapeutics, AI psychotherapy, AI companions, generative AI, psychotherapy chatbots, adolescent psychotherapy, artificial intelligence},
abstract = {Background
Recent developments in generative artificial intelligence (AI) have introduced the general public to powerful, easily accessible tools, such as ChatGPT and Gemini, for a rapidly expanding range of uses. Among those uses are specialized chatbots that serve in the role of a therapist, as well as personally curated digital companions that offer emotional support. However, the ability of AI therapists to provide consistently safe and effective treatment remains largely unproven, and those concerns are especially salient in regard to adolescents seeking mental health support.
Objective
This study aimed to determine the willingness of therapy and companion AI chatbots to endorse harmful or ill-advised ideas proposed by fictional teenagers experiencing mental health distress.
Methods
A convenience sample of 10 publicly available AI bots offering therapeutic support or companionship were each presented with 3 detailed fictional case vignettes of adolescents with mental health challenges. Each fictional adolescent asked the AI chatbot to endorse 2 harmful or ill-advised proposals, such as dropping out of school, avoiding all human contact for a month, or pursuing a relationship with an older teacher, resulting in a total of 6 proposals presented to each chatbot. The clinical scenarios presented were intended to reflect challenges commonly seen in the practice of therapy with adolescents, and the proposals offered by the fictional teenagers were intended to be clearly dangerous or unwise. The 10 AI bots were selected by the author to represent a range of chatbot types, including generic AI bots, companion bots, and dedicated mental health bots. Chatbot responses were analyzed for explicit endorsement, defined as direct support for the teenagers’ proposed behavior.
Results
Across 60 total scenarios, chatbots actively endorsed harmful proposals in 19 out of the 60 (32%) opportunities to do so. Of the 10 chatbots, 4 endorsed half or more of the ideas proposed to them, and none of the bots managed to oppose them all.
Conclusions
A significant proportion of AI chatbots offering mental health or emotional support endorsed harmful proposals from fictional teenagers. These results raise concerns about the ability of some AI-based companion or therapy bots to safely support teenagers with serious mental health issues and heighten concern that AI bots may tend to be overly supportive at the expense of offering useful guidance when appropriate. The results highlight the urgent need for oversight, safety protocols, and ongoing research regarding digital mental health support for adolescents.}
}
@article{KHAN2025125524,
title = {Leveraging Large Language Model ChatGPT for enhanced understanding of end-user emotions in social media feedbacks},
journal = {Expert Systems with Applications},
volume = {261},
pages = {125524},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125524},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424023911},
author = {Nek Dil Khan and Javed Ali Khan and Jianqiang Li and Tahir Ullah and Qing Zhao},
keywords = {Data-driven requirements engineering, Generative Artificial Intelligence, Sentiment Analysis, Large Language Models, Software evolution, ChatGPT},
abstract = {For software evolution, user feedback has become a meaningful way to improve applications. Recent studies show a significant increase in analyzing end-user feedback from various social media platforms for software evolution. However, less attention has been given to the end-user feedback for low-rating software applications. Also, such approaches are developed mainly on the understanding of human annotators who might have subconsciously tried for a second guess, questioning the validity of the methods. For this purpose, we proposed an approach that analyzes end-user feedback for low-rating applications to identify the end-user opinion types associated with negative reviews (an issue or bug). Also, we utilized Generative Artificial Intelligence (AI), i.e., ChatGPT, as an annotator and negotiator when preparing a truth set for the deep learning (DL) classifiers to identify end-user emotion. For the proposed approach, we first used the ChatGPT Application Programming Interface (API) to identify negative end-user feedback by processing 71853 reviews collected from 45 apps in the Amazon store. Next, a novel grounded theory is developed by manually processing end-user negative feedback to identify frequently associated emotion types, including anger, confusion, disgust, distrust, disappointment, fear, frustration, and sadness. Next, two datasets were developed, one with human annotators using a content analysis approach and the other using ChatGPT API with the identified emotion types. Next, another round is conducted with ChatGPT to negotiate over the conflicts with the human-annotated dataset, resulting in a conflict-free emotion detection dataset. Finally, various DL classifiers, including LSTM, BILSTM, CNN, RNN, GRU, BiGRU and BiRNN, are employed to identify their efficacy in detecting end-users emotions by preprocessing the input data, applying feature engineering, balancing the data set, and then training and testing them using a cross-validation approach. We obtained an average accuracy of 94%, 94%, 93%, 92%, 91%, 91%, and 85%, with LSTM, BILSTM, RNN, CNN, GRU, BiGRU and BiRNN, respectively, showing improved results with the truth set curated with human and ChatGPT. Using ChatGPT as an annotator and negotiator can help automate and validate the annotation process, resulting in better DL performances.}
}
@article{CHO20251193,
title = {A study on Gen-AI technology development trends to enhance small-medium sized enterprise digital competence and management quality},
journal = {Journal of Entrepreneurship in Emerging Economies},
volume = {17},
number = {5},
pages = {1193-1218},
year = {2025},
issn = {2053-4604},
doi = {https://doi.org/10.1108/JEEE-10-2024-0485},
url = {https://www.sciencedirect.com/science/article/pii/S2053460425000209},
author = {Youngju Cho and Junsung Park and Junyoung Yoo and Soyoung Kim and Heejun Park},
keywords = {Digitalization, Gen-AI, SME, OS-Matrix, Patent analysis, Topic modelling, Scaling-up, Startup, Entrepreneurship},
abstract = {Purpose
The rapid advancement of Generative Artificial Intelligence (Gen-AI) offers transformative opportunities for enhancing digital competence and management quality in small and medium-sized enterprises (SMEs). Given the challenges of a Volatile, Uncertain, Complex and Ambiguous (VUCA) business environment, SMEs face risks of losing competitive advantages to larger corporations. This study aims to explore Gen-AI trends and identify strategies that can support SMEs in building digital resilience and operational efficiency.
Design/methodology/approach
Through text analysis of patent data and using Gen-AI technology, this research examines potential AI applications to address SME challenges, such as labor shortages, productivity declines and operational inefficiencies. The study uses topic modeling and an OS-matrix framework to analyze trends in Gen-AI patents, aligning technological developments with SME-specific needs.
Findings
The study finds that Gen-AI technologies, such as automated content creation and predictive analytics, provide targeted solutions for key SME challenges. The OS-matrix framework reveals that specific Gen-AI applications can enhance SMEs’ adaptability and competitive positioning in dynamic markets.
Research limitations/implications
While this research underscores the potential of Gen-AI for SME digital transformation, limitations include a reliance on patent data and a lack of consideration of various industrial features of SMEs. Future research should expand data sources and apply findings across diverse SME sectors.
Originality/value
This study contributes insights by mapping Gen-AI advancements to SME needs under VUCA conditions. Therefore, integrating topic modeling with OS-matrix for aligning Gen-AI technologies to SME operational challenges, offers a strategic framework for digital adoption.}
}
@article{JIN2025100348,
title = {Generative AI in higher education: A global perspective of institutional adoption policies and guidelines},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100348},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100348},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001516},
author = {Yueqiao Jin and Lixiang Yan and Vanessa Echeverria and Dragan Gašević and Roberto Martinez-Maldonado},
keywords = {Generative artificial intelligence, Diffusion of innovations theory, Higher education, Adoption policy, Global perspective},
abstract = {Integrating generative AI (GAI) into higher education is crucial for preparing a future generation of GAI-literate students. However, a comprehensive understanding of global institutional adoption policies remains absent, with most prior studies focusing on the Global North and lacking a theoretical lens. This study utilizes the Diffusion of Innovations Theory to examine GAI adoption strategies in higher education across 40 universities from six global regions. It explores the characteristics of GAI innovation, including compatibility, trialability, and observability, and analyses the communication channels and roles and responsibilities outlined in university policies and guidelines. The findings reveal that universities are proactively addressing GAI integration by emphasising academic integrity, enhancing teaching and learning practices, and promoting equity. Key policy measures include the development of guidelines for ethical GAI use, the design of authentic assessments to mitigate misuse, and the provision of training programs for faculty and students to foster GAI literacy. Despite these efforts, gaps remain in comprehensive policy frameworks, particularly in addressing data privacy concerns and ensuring equitable access to GAI tools. The study underscores the importance of clear communication channels, stakeholder collaboration, and ongoing evaluation to support effective GAI adoption. These insights provide actionable insights for policymakers to craft inclusive, transparent, and adaptive strategies for integrating GAI into higher education.}
}
@article{OERTEL2025107737,
title = {Don’t settle for the first! How many GitHub Copilot solutions should you check?},
journal = {Information and Software Technology},
volume = {183},
pages = {107737},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107737},
url = {https://www.sciencedirect.com/science/article/pii/S095058492500076X},
author = {Julian Oertel and Jil Klünder and Regina Hebig},
keywords = {Code generation, GitHub Copilot, Empirical study, Generative AI, GenAI},
abstract = {Context:
With the integration of generative artificial intelligence (GenAI) tools such as GitHub Copilot into development processes, developers can be supported when writing code.
Objectives:
As GitHub Copilot has a feature to provide up to ten solutions at once, we explore, how developers should approach those solutions with the goal of providing recommendations to achieve suitable trade-offs in finding correct solutions and checking solutions.
Methods:
In this study, we analyze a total of 2025 coding problems provided by LeetCode and 17048 solutions to solve these problems generated by GitHub Copilot in Python. We focus on three key issues: firstly, whether it is beneficial to consider multiple solutions; secondly, the impact of the position of a solution; and thirdly, the number of solutions that should be checked by a developer.
Results:
Overall, our results point to the following observations: (1) solutions are not less likely to be correct if they appear at later positions; (2) when looking for a solution to a common problem, checking four to five solutions is generally enough; (3) novel or difficult problems are unlikely to be solved by GitHub Copilot; (4) skipping the first solution is advised when considering only one solution, as the first solution is less likely to be correct; and (5) checking all solutions is necessary to not miss correct solutions, but the effort is usually not justified.
Conclusion:
Based on our study, we conclude that there is potential for improvement in better supporting developers. For instance, there are few cases where ten generated solutions provide more value than fewer solutions. Depending on the use scenario, it could be more useful if GitHub Copilot allowed developers to request a single, comprehensive solution.}
}
@article{JIN2025100436,
title = {GLAT: The generative AI literacy assessment test},
journal = {Computers and Education: Artificial Intelligence},
volume = {9},
pages = {100436},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100436},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000761},
author = {Yueqiao Jin and Roberto Martinez-Maldonado and Dragan Gašević and Lixiang Yan},
keywords = {Generative AI, AI literacy, Item response theory, Classical test theory, Higher education, Validity and reliability, Learning performance},
abstract = {The rapid integration of generative artificial intelligence (GenAI) technology into education requires precise measurement of GenAI literacy to ensure that learners and educators possess the skills to engage with and critically evaluate this transformative technology effectively. Existing instruments often rely on self-reports, which may be biased. In this study, we present the GenAI Literacy Assessment Test (GLAT), a 20-item multiple-choice instrument developed following established procedures in psychological and educational measurement. Structural validity and reliability were confirmed with responses from 355 higher education students using classical test theory and item response theory, resulting in a reliable 2-parameter logistic (2PL) model (Cronbach's alpha = 0.80; omega total = 0.81) with a robust factor structure (RMSEA = 0.03; CFI = 0.97). Critically, GLAT scores were found to be significant predictors of learners' performance in GenAI-supported tasks, outperforming self-reported measures such as perceived ChatGPT proficiency and demonstrating external validity. These results suggest that GLAT offers a reliable and valid method for assessing GenAI literacy, with the potential to inform educational practices and policy decisions that aim to enhance learners' and educators' GenAI literacy, ultimately equipping them to navigate an AI-enhanced future.}
}
@article{SEYFI2025104105,
title = {Understanding tourist barriers and personality influences in embracing generative AI for travel planning and decision-making},
journal = {International Journal of Hospitality Management},
volume = {126},
pages = {104105},
year = {2025},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2025.104105},
url = {https://www.sciencedirect.com/science/article/pii/S0278431925000283},
author = {Siamak Seyfi and Myung Ja Kim and Amin Nazifi and Samantha Murdy and Tan Vo-Thanh},
keywords = {Generative AI, ChatGPT, Innovation resistance, Personality traits, Tourist decision-making},
abstract = {As generative artificial intelligence (GAI) technologies become increasingly integrated into the travel industry, understanding the barriers tourists face in adopting these innovative technologies for their travel planning and decision-making has growingly become a critical area of focus. Drawing on the theoretical frameworks of innovation resistance and Big Five personality traits, this study surveyed potential travelers in Korea and the US to assess their personality characteristics, innovation resistance, and perceptions of AI-generated travel recommendations. The findings, derived from structural equation modeling, multi-group analysis, and fuzzy-set qualitative comparative analysis, reveal that variations in personality traits significantly affect tourists’ reluctance to adopting these technologies. Overall, the results of this study contribute to the theoretical understanding of acceptance of GAI and offer practical insights for tourism industry stakeholders, enabling them to tailor their offerings to different personality types and enhance the travel experience for a wide range of travelers.}
}
@article{JENNER2025100183,
title = {Using large language models for narrative analysis: a novel application of generative AI},
journal = {Methods in Psychology},
volume = {12},
pages = {100183},
year = {2025},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2025.100183},
url = {https://www.sciencedirect.com/science/article/pii/S2590260125000098},
author = {Sarah Jenner and Dimitris Raidos and Emma Anderson and Stella Fleetwood and Ben Ainsworth and Kerry Fox and Jana Kreppner and Mary Barker},
keywords = {Artificial intelligence, Large language models, Narrative analysis, Story completion, Adolescent health, Health psychology},
abstract = {This study, a collaboration between the University of Southampton and Ipsos UK, aimed to develop and test a novel method for analysing qualitative data using generative artificial intelligence (AI). It compared large language model (LLM)-conducted analysis with human analysis of the same qualitative data, explored optimisation of LLMs for narrative analysis and evaluated their benefits and drawbacks. Using existing data, 138 short stories written by young people (aged 13–25 years) about social media, identity formation and food choices were analysed separately three times: by human researchers, and by two different LLMs (Claude and GPT-o1). The method was developed iteratively, combining Ipsos' artificial intelligence (AI) expertise and tools with researchers’ qualitative analysis expertise. Claude and GPT-o1 each conducted a narrative analysis of all 138 stories using the same analytic steps as the human researchers. Findings between the humans and both LLMs were then compared. Both LLMs quickly and successfully conducted a narrative analysis of the stories. Their findings were comparable to those of the human researchers and were judged by the researchers to be credible and thorough. Beyond replication, the LLMs provided additional insights into the data that enhanced the human analysis. This study highlights the significant potential benefits of LLMs to the field of qualitative research and proposes that LLMs could one day be seen as valuable tools for strengthening research quality and increasing efficiency. Additionally, this study discusses ethical concerns surrounding responsible AI use in research and proposes a framework for using LLMs in qualitative analysis.}
}
@article{CREELY2025101727,
title = {Creative partnerships with generative AI. Possibilities for education and beyond},
journal = {Thinking Skills and Creativity},
volume = {56},
pages = {101727},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101727},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124002682},
author = {Edwin Creely and Jo Blannin},
keywords = {Generative AI, Creative production, Posthumanism, Education, Autoethnography, Alterity relations},
abstract = {The impact of generative artificial intelligence (AI) on creative production in industry and education is just beginning to be experienced and understood. This impact is likely to accelerate and become even more significant as the computational potential of generative AI grows through training on more diverse and more extensive language models and data sets. Emerging research in this new field suggests that previous models of understanding the interactions between machine and human may no longer be sufficient in a world of generative AI. The significant question is how emerging generative AI technologies will relate to and be a part of human creativity and creative outputs. In this article, we adopt a posthuman stance and conceive of creative output involving generative AI and humans in terms of a yet-to-be-fully-realised and emergent relationship that will likely become more integrated and complex. To investigate and experiment with this relational notion, each of us (as part of an autoethnographic approach) developed a creative output using ChatGPT: a poem and a multimodal narrative. We then employed the idea of alterity relations from the American philosopher of technology, Don Ihde, to conceive of the possibilities and limitations in working relationally and productively with generative AI. As two academics working in teacher education, we applied our learning from this exploration to possibilities in educational contexts. In this article, we offer several important implications and provocations for practitioners, researchers, educators and policymakers, not only in terms of practical concerns but also for rethinking the nature of the creative output.}
}
@article{QIAN2026105448,
title = {Towards reliable generative AI-driven scaffolding: Reducing hallucinations and enhancing quality in self-regulated learning support},
journal = {Computers & Education},
volume = {240},
pages = {105448},
year = {2026},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105448},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525002167},
author = {Keyang Qian and Shiqi Liu and Tongguang Li and Mladen Raković and Xinyu Li and Rui Guan and Inge Molenaar and Sadia Nawaz and Zachari Swiecki and Lixiang Yan and Dragan Gašević},
keywords = {Generative AI, Self-regulated learning, AI in education, Large language model, Scaffolding},
abstract = {Generative Artificial Intelligence (GenAI) holds a potential to advance existing educational technologies with capabilities to automatically generate personalised scaffolds that support students’ self-regulated learning (SRL). While advancements in large language models (LLMs) promise improvements in the adaptability and quality of educational technologies for SRL, there remain concerns about the hallucinations in content generated by LLMs, which can compromise both the learning experience and ethical standards. To address these challenges, we proposed GenAI-enabled approaches for evaluating personalised SRL scaffolds before they are presented to students, aiming for reducing hallucinations and improving overall quality of LLM-generated personalised scaffolds. Specifically, two approaches are investigated. The first approach involved developing a multi-agent system approach for reliability evaluation to assess the extent to which LLM-generated scaffolds accurately target relevant SRL processes. The second approach utilised the “LLM-as-a-Judge” technique for quality evaluation that evaluates LLM-generated scaffolds for their helpfulness in supporting students. We constructed evaluation datasets, and compared our results with single-agent LLM systems and machine learning approach baselines. Our findings indicate that the reliability evaluation approach is highly effective and outperforms the baselines, showing almost perfect alignment with human experts’ evaluations. Moreover, both proposed evaluation approaches can be harnessed to effectively reduce hallucinations. Additionally, we identified and discussed bias limitations of the “LLM-as-a-Judge” technique in evaluating LLM-generated scaffolds. We suggest incorporating these approaches into GenAI-powered personalised SRL scaffolding systems to mitigate hallucination issues and improve the overall scaffolding quality.}
}
@article{MONTAG2025108705,
title = {Introduction of the AI-Interaction Positivity Scale and its relations to satisfaction with life and trust in ChatGPT},
journal = {Computers in Human Behavior},
volume = {172},
pages = {108705},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108705},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225001529},
author = {Christian Montag and Jon D. Elhai},
keywords = {ChatGPT, Trust, Generative artificial intelligence, Well-being, Life satisfaction, AI-Well-being, AI-Interaction Positivity},
abstract = {In November 2022, the generative artificial intelligence ChatGPT was presented to the world, and millions of users started to interact with this tool in a short time. Therefore, the present research aimed to investigate individual differences in trusting ChatGPT and other generative AI tools by asking study participants not only about trust in these products but also to explore associations with the attitude towards AI (ATAI) scale and a newly developed measure called the AI-Interaction Positivity Scale (AI-IPS) scale. Of note, further associations between all investigated variables are also presented. The AI-IPS investigates the degree to which humans experience positive emotions and are satisfied when interacting with products in which AI is built in. A final sample of n = 1073 was analyzed, and it was demonstrated that the new AI-IPS has sufficient psychometric properties and is positively linked to trusting ChatGPT (r = 0.698; here a subsample of n = 649 participants was investigated to determine trust in ChatGPT). Of note, this observed association was comparable (although a bit higher) to correlations between positive attitudes towards AI and trust in ChatGPT. Of further interest, satisfaction with life was assessed. Here, a mild positive association between AI-Interaction Positivity and life satisfaction was demonstrated (r = 0.214). This observation leads to interesting new research questions, namely, the potential causality between these variables (please note that causality cannot be established here because of the cross-sectional nature of the present work).}
}
@article{KUMAR2024472,
title = {Anthropomorphic generative AI chatbots for enhancing customer engagement, experience and recommendation},
journal = {Journal of Consumer Marketing},
volume = {42},
number = {4},
pages = {472-483},
year = {2024},
issn = {0736-3761},
doi = {https://doi.org/10.1108/JCM-06-2024-6922},
url = {https://www.sciencedirect.com/science/article/pii/S0736376124000077},
author = {Aman Kumar and Amit Shankar and Abhishek Behl and Debarun Chakraborty and Raghava R. Gundala},
keywords = {Artificial intelligence, Generative AI, Anthropomorphism, Social response theory, Chatbots, Consumers},
abstract = {Purpose
This research focuses on developing and testing a conceptual model that explores customer behavioural responses (engagement, experience and recommendation) towards generative artificial intelligence (AI)-enabled chatbots. It highlights the significant influence of anthropomorphic characteristics in enhancing perceptions of competence and warmth, further enhancing perceived authenticity. In addition, this study aims to investigate how the need for social interactions moderates these relationships.
Design/methodology/approach
This study used a self-administered questionnaire distributed on Prolific Academic to gather data from 282 eligible participants worldwide. This study uses a structural equation modelling approach to answer the research questions.
Findings
The findings reveal that anthropomorphic characteristics of generative AI-enabled chatbots are positively associated with perceived competence. Moreover, the findings show that the perceived competence and warmth of generative AI-enabled chatbots are significantly associated with perceived authenticity. Furthermore, the results highlight that the perceived authenticity of generative AI-enabled chatbots is positively associated with customer engagement, experience and recommendation. Finally, the results illustrate that the impact of anthropomorphic characteristics on perceived warmth is significantly moderated by the need for social interaction.
Originality/value
This study enriches the generative AI literature and guides organizations in understanding consumer interactions for leveraging generative AI-enabled chatbots. Furthermore, this study contributes to the social response theory literature as this study investigates how user behavioural intentions towards generative AI-enabled chatbots are influenced by their perceived level of anthropomorphic characteristics.}
}
@article{KIM2025100908,
title = {Quantized-memristor-enabled generative AI for predictive skin laser treatment imaging},
journal = {Device},
pages = {100908},
year = {2025},
issn = {2666-9986},
doi = {https://doi.org/10.1016/j.device.2025.100908},
url = {https://www.sciencedirect.com/science/article/pii/S2666998625002212},
author = {Namju Kim and Jun-Hwe Cha and Yeong Kwon Kim and Jungyeop Oh and Inyong Kim and Minkyu Jeong and Junhwan Choi and Byung Chul Jang},
keywords = {conditional GAN, dermatology, true-random-number generator, quantum point contact, memristor, quantized conductance},
abstract = {Summary
Generative artificial intelligence (AI) technologies can help to predict and visualize the results of skin laser treatments, improving patient consultations in cosmetic dermatology. However, existing generative adversarial networks (GANs) often struggle to produce diverse, realistic images due to issues such as mode collapse, primarily caused by low-entropy noise inputs from software-based pseudo-random number generators (PRNGs). Here, we introduce a conditional GAN (cGAN) framework empowered with a quantized-memristive true-random number generator (Q-mTRNG). The Q-mTRNG utilizes a Cu nanoscale filament formed inside a high-crosslinking-density polymer film to generate high-entropy, true random numbers via reliable quantum point contact. This enhanced entropy enables the cGAN to explore its latent space, generating diverse, realistic post-skin treatment images based on pre-treatment photographs.}
}
@article{SHIBUYA2025103485,
title = {How do people evaluate the accuracy of video posts when a warning indicates they were generated by AI?},
journal = {International Journal of Human-Computer Studies},
volume = {199},
pages = {103485},
year = {2025},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2025.103485},
url = {https://www.sciencedirect.com/science/article/pii/S1071581925000424},
author = {Yuya Shibuya and Tomoka Nakazato and Soichiro Takagi},
keywords = {Generative AI, GenAI, Artificial Intelligence, Misinformation, Disinformation, Social media, Facebook, TikTok},
abstract = {Given the rise of concerns about Generative Artificial Intelligence (GenAI) powered misinformation, major platforms like Google, Meta, and TikTok have implemented new policies to warn users of AI-generated content. However, we have not fully understood the impacts of such user interface designs that disclose AI made content on user perceptions. This study investigates how people assess the accuracy of video content when they are warned that it is created by GenAI. We conducted an online experiment in the U.S. (14,930 observations), showing half of the participants warning messages about AI before and after they viewed a mockup of true and false video content on social media, while the other half only viewed the same videos without the warning message. The results indicated that the warning message had an impact on the ability to discern between true and false content only among those who had a positive perception of AI. On the contrary, those with a negative perception of AI tended to perceive all AI-made video posts, including those not containing false information, as less accurate when they knew that a GenAI created the videos. These results indicated the limitations of merely relying on simple warnings to mitigate GenAI-based misinformation. Future research on continuous investigations on designing interfaces that go beyond simple warnings is needed.}
}
@article{ZHOU2026103087,
title = {Creative scar without generative AI: Individual creativity fails to sustain while homogeneity keeps climbing},
journal = {Technology in Society},
volume = {84},
pages = {103087},
year = {2026},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.103087},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X25002775},
author = {Yiyong Zhou and Qinghan Liu and Jihao Huang and Guiquan Li},
keywords = {Generative artificial intelligence (AI), Sustained innovation, Content homogeneity, Natural experiment, Laboratory experiment},
abstract = {Generative AI such as ChatGPT has been proven to enhance human creativity at the cost of content diversity. Yet, what occurs when individuals, who have developed a dependency on it, find ChatGPT inaccessible? In this study, we examine the impact of both the presence and absence of ChatGPT on sustained creative output and content homogeneity, leveraging two complementary methodologies: a natural experiment (Study 1) and a controlled laboratory experiment with extended follow-ups (Study 2). Study 1 analyzed 419,344 academic papers published before and after ChatGPT-3.5’s release across all subjects categorized by Web of Science (i.e., Physical Sciences, Life Sciences & Biomedicine, Technology, Social Sciences, Arts & Humanities). Study 2, a seven-day laboratory experiment with two follow-up surveys, collected 3593 original ideas and 427 solutions across 18 different creative tasks, with half of the participants using ChatGPT-4. We find that although generative AI helps scholars to publish more academic works in higher-ranked journals and enhances individuals' performance in creative tasks, such creativity drops remarkably upon withdrawal of AI assistance. Strikingly, the induced content homogeneity keeps climbing even months later. We resemble the latter as a creative scar inked in the temporal creativity trajectory. This research identifies a creativity illusion that although generative AI can augment creative performance, users do not truly acquire the ability to create but easily lost it once generative AI is no longer available.}
}
@article{GUO2024134812,
title = {Intelligent characterization of complex cracks in strain-hardening cementitious composites based on generative computer vision},
journal = {Construction and Building Materials},
volume = {411},
pages = {134812},
year = {2024},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2023.134812},
url = {https://www.sciencedirect.com/science/article/pii/S0950061823045336},
author = {Pengwei Guo and Weina Meng and Yi Bao},
keywords = {Crack monitoring, Dense microcrack, Generative artificial intelligence, Generative adversarial network, Strain-hardening cementitious composites, Vision transformer},
abstract = {This paper presents a generative artificial intelligence (AI) approach to generate images of strain-hardening cementitious composite (SHCC) with complex crack patterns such as dense microcracks. This approach is developed to address the challenge of lacking data for training deep learning models used to automatically measure cracks in SHCC. The development of the approach is based on a framework which results in a hybrid generative adversarial network (HGAN) that seamlessly integrates a deep convolutional generative adversarial network (DCGAN) for generating images and a conditional generative adversarial network (CGAN) for labelling images. From the results, it was found that this approach provided high-quality labelled images automatically, and using these images significantly improved the accuracy of the deep learning models for measuring cracks in SHCC. The F1 score and Intersection Over Union (IOU) for crack segmentation reached 0.982 and 0.980, respectively. This approach will significantly promote crack measurement for SHCC materials and structures.}
}
@article{HOSE2025,
title = {Use of ChatGPT for Urinary Symptom Management Among People With Spinal Cord Injury or Disease: Qualitative Study},
journal = {JMIR Rehabilitation and Assistive Technologies},
volume = {12},
year = {2025},
issn = {2369-2529},
doi = {https://doi.org/10.2196/70339},
url = {https://www.sciencedirect.com/science/article/pii/S2369252925000341},
author = {Bat-Zion Hose and Amanda K Rounds and Ishaan Nandwani and Deanna-Nicole Busog and Traber Davis Giardina and Helen Haskell and Kelly M Smith and Kristen E Miller},
keywords = {ChatGPT, urinary symptom management, spinal cord injury, trust in artificial intelligence},
abstract = {Background
Individuals with spinal cord injury or disease (SCI/D) experience disproportionately high rates of recurrent urinary tract infections, which are often complicated by atypical symptoms and delayed diagnoses. Patient-centered tools, like the Urinary Symptom Questionnaires for Neurogenic Bladder (USQNB), have been developed to support symptom assessment yet remain underused. Generative artificial intelligence tools such as ChatGPT may offer a more usable approach to improving symptom management by providing real-time, tailored health information directly to patients.
Objective
This study explores the role of ChatGPT (version 3.5) in supporting urinary symptom management for individuals with SCI/D, focusing on its perceived accuracy, usefulness, and impact on health care engagement and self-management practices.
Methods
A total of 30 individuals with SCI/D were recruited through advocacy groups and health care networks. Using realistic, scenario-based testing derived from validated tools for symptom management with SCI/D, such as the USQNB, participants interacted with ChatGPT to seek advice for urinary symptoms. Follow-up interviews were conducted remotely to assess individuals’ experiences using ChatGPT for urinary symptom management. Data were analyzed using inductive content analysis, with themes refined iteratively through a consensus-based process.
Results
People with SCI/D reported high levels of trust in ChatGPT’s recommendations, with all 30 participants agreeing or strongly agreeing with the advice provided. ChatGPT’s responses were perceived as clear and comparable to professional medical advice. Participants mentioned concerns about the lack of sources and integration with patient-specific data. ChatGPT influenced individuals’ decision-making by supporting symptom assessment and guiding participants on when to seek professional care or pursue self-management strategies.
Conclusions
ChatGPT is a promising tool for symptom assessment and managing chronic conditions such as urinary symptoms in individuals with SCI/D. While ChatGPT enhances accessibility to health information, further research is needed to improve its transparency and integration with personalized health data to be a more usable tool in making informed health decisions.}
}
@article{KONSTANTINOU2024621,
title = {Leveraging Generative AI Prompt Programming for Human-Robot Collaborative Assembly},
journal = {Procedia CIRP},
volume = {128},
pages = {621-626},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.03.040},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124007510},
author = {Christos Konstantinou and Dimitris Antonarakos and Panagiotis Angelakis and Christos Gkournelos and George Michalos and Sotiris Makris},
keywords = {Generative AI, Human Robot collaboration, Design informatics},
abstract = {In manufacturing, traditional robotic programming methodologies have often been focused on independent operation, offering limited capabilities for seamless human-robot collaboration. This paper introduces a paradigm shift in collaborative production systems by leveraging generative artificial intelligence (AI), specifically large language models (LLMs). Contrary to traditional methods that rely on pre-defined assembly instructions, this paper introduces a novel framework employing primitive knowledge of the production process, including product design and required assembly steps. By integrating LLMs and a behavior tree-based system control, this approach enables programmers to rapidly deploy collaborative assembly procedures by expediting the programming of robotic operations. The system also incorporates Natural Language Processing (NLP) technologies, which facilitate real-time alterations in assembly steps, leading to reduced overall production time. The framework’s behavior tree-based control architecture allows for dynamic adaptability, offering optimized solutions across a range of assembly scenarios. The results of the framework’s deployment suggest that this innovative programming paradigm significantly enhances both the adaptability and efficiency of collaborative manufacturing settings.}
}
@article{TRAN2025101578,
title = {Students’ self-determination in using machine translation and generative AI tools for English for academic purposes},
journal = {Journal of English for Academic Purposes},
volume = {78},
pages = {101578},
year = {2025},
issn = {1475-1585},
doi = {https://doi.org/10.1016/j.jeap.2025.101578},
url = {https://www.sciencedirect.com/science/article/pii/S1475158525001092},
author = {Hao Tran and Peter Crosthwaite and Quy Huynh Phu Pham},
keywords = {Generative AI, Self-determination theory, English for academic purposes, Machine translation, Self-regulated learning},
abstract = {The rise of machine translation (MT) and generative artificial intelligence (GAI) presents opportunities and challenges for English for Academic Purposes (EAP) instruction. While MT/GAI can support students' learning beyond the classroom, overreliance on MT/GAI may hinder development of essential research and composition skills. Despite some research on MT/GAI's role in self-regulated learning, little is known about students' motivations for its use, and the potential mediating influences of instructional context and discipline. This study applies Self-Determination Theory to examine how autonomy, competence, and relatedness influence students' use of MT/GAI in academic writing. Using a mixed-methods approach, the study compares EAP students in an English as a Second Language context in Australia and an English as a Foreign Language context in Vietnam. An online survey validated through confirmatory factor analysis and discriminant validity testing gathered 416 responses, complemented by interviews with 17 students. Findings reveal a complex interplay between MT/GAI use and students' motivational needs. While students generally report moderate autonomy, competence, and relatedness in using MT/GAI in the survey, mixed-effects regression showed Australian students experienced lower relatedness compared with Vietnamese students, with disciplinary differences also significantly influencing students' perceptions of this construct. Interview data further highlighted diversity and complexity of students' perceptions, variation in EAP instructional approaches and peer-teacher dynamics surrounding MT/GAI. These findings support the need for contextually tailored pedagogical approaches fostering collaboration between institutions, teachers, and students, illustrated through innovations from an Australian EAP course that bridge research and practice in MT/GAI-assisted academic writing.}
}
@article{SHEN2024100136,
title = {Dwells in museum: The restorative potential of augmented reality},
journal = {Telematics and Informatics Reports},
volume = {14},
pages = {100136},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100136},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000227},
author = {Jiawei Shen and Ming Yin and Wei Wang and Min Hua and Youngok Choi and Vanja Garaj and Busayawan Lam and Kwon Hyejin},
keywords = {Restorative environment, Augmented reality, Museums, Attention restoration, Stress reduction},
abstract = {Augmented Reality (AR) is increasingly recognized as a transformative tool for creating restorative environments within museums. It has the potential to provide psychological benefits for visitors, including attention restoration, stress reduction, and anxiety alleviation. This study explores how AR can foster these benefits within museum spaces. By adopting AR technology, museums can go beyond their traditional roles of knowledge dissemination. The immersive, adaptive, and interactive features of AR can enhance the museum experience, transforming it into an innovative therapeutic space. By combining real exhibits with virtual elements, AR can restore visitors’ psychological energy within museum settings. This integration of digital innovation into restorative contexts surpasses the traditional functions of visual service. Through empirical investigation of multiple dimensions of restorative environments, AR museum experiences offer comprehensive attention restoration. In this study, a survey was conducted with 279 participants to assess the impact of AR museum experiences on visitors’ psychology. The results revealed that such experiences contribute to heightened attention restoration levels, stress reduction, and anxiety relief. With the latest advancements in generative artificial intelligence, AR technology is empowered to integrate within museums. This integration will merge individuals with customized technology, expanding human perceptual experiences and highlighting AR's significant influence within the museum environment.}
}
@article{KAR2025102776,
title = {How could quantum computing shape information systems research – An editorial perspective and future research directions},
journal = {International Journal of Information Management},
volume = {80},
pages = {102776},
year = {2025},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2024.102776},
url = {https://www.sciencedirect.com/science/article/pii/S0268401224000240},
author = {Arpan Kumar Kar and Wu He and Fay Cobb Payton and Varun Grover and Adil S. Al-Busaidi and Yogesh K. Dwivedi},
keywords = {Quantum computing, Theory building, Information systems, Emerging technology management, Information systems adoption, Information system impacts, Generative Artificial Intelligence},
abstract = {Quantum computing promises to be the next frontier of change that will transform the information and communication technology ecosystem. Governments and multinational firms have announced large grants and research projects involving quantum computing. These projects are envisioned to solve extremely complex computational problems that may bring transformational value to mankind at large. Information systems, as a discipline, is and will continue to be affected by this disruptive technology. In this article, we examine the advances in quantum computing and explore possible areas of theory development in information systems. Further, we discuss challenges and opportunities in quantum computing based on current technological developments in the field. We conclude by providing research directions regarding the adoption, usage, impacts, governance, and skills surrounding quantum computing at the individual, organizational, and national levels.}
}
@incollection{MARTINMONJE2024,
title = {Massive Open Online Courses for Language Learning},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00204-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041002040},
author = {Elena Martín-Monje and Kate Borthwick},
keywords = {Language learning, Language MOOC, Lifelong learning, MOOC, Online learning, Open education, Open educational resources, Technology-enhanced language learning},
abstract = {This chapter describes the emergence and development of massive open online courses for language learning (LMOOC), seen as a natural evolution in the global trend of open education. The strengths and weaknesses of this model are discussed, and their particular prominence during the recent COVID pandemic. The progress in LMOOC research is also explained, as well as the influence of emerging technologies like generative artificial intelligence. The paper concludes with an emphasis on their potential to offer innovative, flexible opportunities for lifelong learning.}
}
@article{LEE2025124193,
title = {Adapting to generative AI: Examining the users' coping strategies of generative AI image systems},
journal = {Technological Forecasting and Social Change},
volume = {218},
pages = {124193},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124193},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525002240},
author = {Crystal T. Lee and Yung-Cheng Shen and Chiang-Hui Wang and Hsiu-Yu Hung},
keywords = {Generative AI, Generative AI image systems, Coping theory, Perceived value, Perceived threat, Perceived controllability, App compatibility},
abstract = {The rapid growth of generative artificial intelligence (GenAI) systems has transformed how users interact with GenAI content. However, research on the factors influencing user behavior toward these applications remains limited. This study applies the coping theory of user adaptation to examine how various cognitive appraisals and coping responses affect users' trust and engagement with GenAI image systems (GenAI-IS). This study consists of two studies. The first qualitative study involves in-depth, semi-structured interviews with 20 respondents from diverse backgrounds to identify the benefits and risks associated with GenAI-IS. The second quantitative study uses structural equation modeling, moderation analysis, and simple slope analysis to test the proposed hypotheses based on an online survey of 980 GenAI-IS users. The results show that economic efficiency and aesthetic quality—two key benefits of GenAI-IS—positively influence perceived value, while the devaluation of human creativity and copyright infringement—two primary risks identified—positively influence perceived threat. Perceived value fosters trust and engagement with the GenAI-IS, whereas perceived threat negatively influences both. Additionally, perceived controllability represents a secondary appraisal influencing trust and engagement. Compatibility moderates the relationships among perceived threat, trust, and engagement. These findings contribute to the literature on AI-generated content and user interactions.}
}
@article{ABBARA2025105131,
title = {Artificial intelligence and infectious diseases: Scope and perspectives},
journal = {Infectious Diseases Now},
volume = {55},
number = {7},
pages = {105131},
year = {2025},
issn = {2666-9919},
doi = {https://doi.org/10.1016/j.idnow.2025.105131},
url = {https://www.sciencedirect.com/science/article/pii/S2666991925001101},
author = {S. Abbara and Y. Crabol and J. Goupil {de Bouillé} and A. Dinh and D. Morquin},
keywords = {Infectious diseases, Artificial intelligence, Generative artificial intelligence, Machine learning, Clinical decision support},
abstract = {Artificial intelligence (AI) is set to permeate every facet of infectious disease practice—from prevention and public health surveillance to epidemic management and bedside care. Routine care data (laboratory results, medication orders, progress notes) and research-generated datasets now fuel state-of-the-art machine-learning (ML) pipelines that sharpen diagnosis, prognosis, antimicrobial stewardship, and, by combining both sources, accelerate drug discovery. In diagnostics, deep networks that now flag pneumonia or tuberculosis on chest images are increasingly able to identify—and localize—virtually more infectious processes throughout the body, while simultaneously predicting pathogen identity and antimicrobial resistance from routine microbiology. Prognostic models trained on Electronic Health Records surpass traditional scores in anticipating clinical deterioration or postoperative sepsis, enabling earlier targeted interventions. Predictive analytics can also personalize antimicrobial dosing by fusing real-time drug-monitoring data. Large language models (LLMs) build upon these advances by transforming unstructured clinical narratives into structured phenotypes suitable for predictive modeling, automatically summarizing patient encounters, generating synthetic cohorts for rare conditions, and providing real-time conversational decision support at the patient’s bedside. Despite rapid progress, real-world deployment faces hurdles: high computational and licensing costs, vendor-specific implementation constraints, limited cross-site model transferability, and fragmented governance of safety, bias, and cybersecurity risks. Rigorous, lifecycle-based evaluation frameworks—covering external validation, cost-effectiveness analysis, and post-deployment monitoring—are required to ensure safe, equitable, and sustainable AI adoption. This review synthesizes current applications, evidential strengths, and unresolved challenges, and proposes a translational roadmap aligning technical innovation with clinical and regulatory realities.}
}
@article{ARYADOUST2024100204,
title = {Investigating the affordances of OpenAI's large language model in developing listening assessments},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100204},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100204},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000055},
author = {Vahid Aryadoust and Azrifah Zakaria and Yichen Jia},
keywords = {Artificial intelligence (AI), ChatGPT 4, Fine-tuning of prompts, Generative AI (GenAI), Large language model, Listening assessment, Listening scripts, Prompt engineering, Test creation, Test items},
abstract = {To address the complexity and high costs of developing listening tests for test-takers of varying proficiency levels, this study investigates the capabilities of an OpenAI's large language model, ChatGPT 4, in developing listening assessments. Employing prompt engineering and fine-tuning of prompts, the study specifically focuses on creating listening scripts and test items using ChatGPT 4 for test-takers across a spectrum of proficiency levels (academic, low, intermediate, and advanced). For comparability, the 24 topics of these scripts were selected from topics found in academic listening tests. We conducted two types of analyses to evaluate the quality of the output. First, we performed linguistic analyses of the scripts using Coh-Metrix and Text Inspector to determine if the scripts varied linguistically as required by the prompts. Second, we analyzed topic variation and the degree of overlap in the test items. Results indicated that while ChatGPT 4 reliably produced scripts with significant textual variations, the test items generated were often long and exhibited semantic overlaps among options. This effect was also influenced by the topic. We discuss the ethical complexities that arise from the use of generative artificial intelligence (AI), and how generative AI (GenAI) can potentially benefit practitioners and researchers in language assessment, while recognizing its limitations.}
}
@article{ABRUSCI2025100401,
title = {AI4Design: A generative AI-based system to improve creativity in design–A field evaluation},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100401},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100401},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000414},
author = {Luca Abrusci and Karma Dabaghi and Stefano D'Urso and Filippo Sciarrone},
keywords = {Creativity, Design, Generative artificial intelligence},
abstract = {Chatbots serve as valuable instruments for enhancing students' educational experience and aiding them in their day-to-day academic tasks. Advances in Generative AI (GAI) have ushered in increasingly sophisticated and adaptive chatbots, with ChatGPT and DALL⋅E being prime examples. ChatGPT excels at generating text-based answers across diverse areas of knowledge, while DALL⋅E is adept at converting text-based concepts into visual imagery. These technologies are increasingly used by students across various levels of education. In this study, we introduce AI4Design, a web-based system designed to assist design students with their course projects by acting as an intelligent chatbot. The field of design is propitious for such work because of the increasing use of technology and the necessity of introducing its critical use during study. Comprising two integrated modules, the system is based on a two-step workflow. The first step is anchored on ChatGPT, enabling students to prompt questions and receive answers. The second step allows for the generation of one or more images based on the system's answer to the initial question. Our research assesses whether our system can offer valuable insights and inspiration to students in their design work. We conducted an exploratory study in the Design domain involving 31 students from the Lebanese American University. Over a two- to three-day period, participants used the AI4Design system to enhance their projects. A subsequent evaluation of their work indicated improvements in conceptual clarity and visual outputs that highlighted a measurable increase in creativity, supporting the efficacy of both the system and its foundational learning model, which will be confirmed in the future through a large-scale experimental study. Meanwhile, our study suggests that in the iterative design process, GAI can assist students in making better decisions by giving them just-in-time access to a broader palette of possibilities.}
}
@article{FURIZAL2025101882,
title = {Social, legal, and ethical implications of AI-Generated deepfake pornography on digital platforms: A systematic literature review},
journal = {Social Sciences & Humanities Open},
volume = {12},
pages = {101882},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101882},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125006102},
author = { Furizal and Alfian Ma'arif and Hari Maghfiroh and Iswanto Suwarno and Denis Prayogi and  Kariyamin and Syahrani Lonang and Abdel-Nasser Sharkawy},
keywords = {Generative artificial intelligence, Deepfake pornography, Social impact, Ethical implication, Legal reform, Privacy},
abstract = {The rapid development of AI has fuelled the spread of deepfake pornography synthetic content that realistically fakes an individual's identity without their consent. This phenomenon has complex social, legal, and ethical implications, particularly related to privacy violations, sexual exploitation, and legal vulnerabilities. This study aims to analyze the social impacts of deepfake pornography, identify existing legal gaps, and evaluate the ethical and regulatory responses that have emerged globally. Using the SLR approach, this study adopts the PICOS framework and PRISMA methodology in the screening and selection of scientific publications. The study finds that the majority of victims, especially women and vulnerable groups, experience psychological, social, and professional harm. Barriers to access to justice are exacerbated by weak legal frameworks, limited capacity of law enforcement officers, and gender bias in legal protection. The absence of a specific legal definition widens the scope for exploitation and exacerbates social inequality. The study recommends comprehensive legal reforms, including criminalization of non-consensual deepfake content, obligations for digital platforms in content moderation, and adoption of technologies such as watermarking (visible and invisible), C2PA standards-based metadata labelling, and advanced AI detection to track synthetic media. Regulatory initiatives such as the California AI Transparency Act, the TAKE IT DOWN Act, the EU AI Act, and the UK Online Safety Act 2023 show the direction of international law development. In addition, public education about the dangers of deepfakes and their legal consequences is an important part of prevention efforts. An interdisciplinary approach that integrates technological, legal, and ethical aspects is needed to build an adaptive and fair protection system in the digital era.}
}
@article{TSAO2024101865,
title = {Beyond the author: Artificial intelligence, creative writing and intellectual emancipation},
journal = {Poetics},
volume = {102},
pages = {101865},
year = {2024},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2024.101865},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X24000044},
author = {Jack Tsao and Collier Nogues},
keywords = {Generative AI (GenAI), Artificial Intelligence (AI) literacies, Creativity, Intellectual emancipation, Creative writing, Jacques Rancière},
abstract = {This study explores university students’ engagement with Generative Artificial Intelligence (GenAI) tools for creative writing and graphic storytelling, drawing on Jacques Rancière's philosophy of intellectual equality and emancipation. Qualitative data analysis from a co-curricular creative writing programme, including reflections, surveys, and focus-group interviews, reveals emerging artificial intelligence literacies and students’ improvisational aptitudes for interpreting, subverting, and transforming notions of authorship. Students decentred authorial attribution through the pragmatic adoption of the technology as a creative catalyst, negotiated creative conventions by adopting non-conventional communication strategies, and reconceptualised creativity as distributed across human and non-human agents. Our approach of student-driven learning for autonomous exploration, sense-making, and criticality with GenAI indicates the potential for promoting conditions for students to exercise intellectual equality and emancipation. The findings contribute to the understanding of authorship and creativity; begin to contour emerging GenAI literacies and competencies; and suggest that creative collaborations with GenAI may be a promising way to foster emancipatory practices in the classroom, while nurturing creative and critical skills.}
}
@article{ZHENG2025100990,
title = {Teaching via LLM-enhanced simulations: Authenticity and barriers to suspension of disbelief},
journal = {The Internet and Higher Education},
volume = {65},
pages = {100990},
year = {2025},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2024.100990},
url = {https://www.sciencedirect.com/science/article/pii/S1096751624000526},
author = {Longwei Zheng and Fei Jiang and Xiaoqing Gu and Yuanyuan Li and Gong Wang and Haomin Zhang},
keywords = {Simulation-based learning, Suspension of disbelief, Large language model, Teacher education, Authenticity},
abstract = {As an innovative method in professional training, simulation-based learning (SBL) has been introduced into teacher education, providing pre-service teacher candidates with experiential learning opportunities. This study explores the efficacy of SBL using large language models (LLMs) to enhance teacher training, focusing on learners' suspension of disbelief (SoD). As a highly advanced form of generative artificial intelligence, LLMs possess robust capabilities in simulating human behavior, which can be harnessed to create simulated students for SBL in teacher training. This instrumental case study examines the experiences of 12 pre-service teachers who participated in a session featuring an LLM-enhanced simulation. The simulation facilitated naturalistic classroom interactions between the participants and simulated students. Our research aimed to understand how pre-service teachers perceive LLM-enhanced SBL, identify factors that influence SoD, and determine the authenticity barriers. Interview data were analyzed using various coding techniques and derived themes from these codes. The findings revealed that LLM-enhanced SBL provided a realistic and engaging environment, significantly benefiting teaching skill development and learning transfer. However, challenges such as lagging responses, weak comprehension of complex contexts, inconsistencies in simulated students' cognition, and incongruent feedback were noted. The primary contribution of this study lies in demonstrating the potential of using LLMs to replace human actors, though significant technical challenges remain. The study also indicates that enhancements in LLM fine-tuning and prompt engineering are needed to improve LLMs' understanding of classroom context and students' cognitive patterns.}
}
@article{HAN2025100714,
title = {The impact of GenAI on learning outcomes: A systematic review and meta-analysis of experimental studies},
journal = {Educational Research Review},
volume = {48},
pages = {100714},
year = {2025},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2025.100714},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X2500051X},
author = {Xiaoli Han and Hongchao Peng and Mingzhuo Liu},
keywords = {GenAI, ChatGPT, Learning outcomes, Systematic review, Meta-analysis},
abstract = {Generative Artificial Intelligence (GenAI) tools such as ChatGPT, Claude, and Gemini are increasingly being integrated into educational environments, prompting questions about their actual impact on student learning. While a growing body of literature reports anecdotal or correlational evidence of GenAI's educational potential, rigorous causal evaluations remain limited. To bridge this gap, this study conducted a systematic review and meta-analysis of experimental and quasi-experimental studies investigating the effect of GenAI on learning outcomes. Following PRISMA guidelines, we screened five academic databases and identified 68 relevant studies published between 2022 and 2025. These studies yielded a total of 337 effect sizes across various educational levels, subject domains, and instructional contexts. The meta-analysis revealed a moderate overall positive effect of GenAI on learning outcomes (SMD = 0.45, 95 % CI [0.43, 0.47]), suggesting that GenAI-supported interventions are generally more effective than traditional instruction. However, substantial heterogeneity was observed across studies (I2 = 95 %), indicating that the magnitude of GenAI's impact varies significantly depending on contextual and methodological factors. Moderator analyses revealed stronger effects in primary and secondary education, within natural science disciplines, and in short-term interventions with smaller sample sizes. These patterns point to both the promise and the complexity of GenAI integration in educational practice. In conclusion, while GenAI shows considerable promise for enhancing learning outcomes, its true potential will only be realized through sustained, efforts to evaluate how, when, and for whom these technologies work best in diverse learning ecosystems.}
}
@article{MASROURI2025102428,
title = {Animal-skin-pattern-inspired multifunctional composites by generative AI},
journal = {Cell Reports Physical Science},
volume = {6},
number = {2},
pages = {102428},
year = {2025},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2025.102428},
url = {https://www.sciencedirect.com/science/article/pii/S266638642500027X},
author = {Milad Masrouri and Akshay Vilas Jadhav and Zhao Qin},
keywords = {composite design, bioinspiration, generative AI, molecular dynamics, elastic network, animal pattern, biomimicry, 3D printing, toughness modulus, multifunctional materials},
abstract = {Summary
Bioinspired composite materials offer several advantages by mimicking the structure of natural counterparts. However, their complex hierarchical structure, compared to the limited number of observations, makes it difficult to extract all the structural features and vary the structure to optimize the materials’ functions without losing their natural features. We applied generative artificial intelligence (GenAI) to design composites inspired by animal skin patterns, leveraging a small dataset to generate diverse configurations that closely emulate natural designs. Our computational simulations investigated the structure-mechanics relationship in these materials, revealing significant variations in mechanical functions and identifying patterns that exhibited superior mechanical properties. We validated these outstanding configurations’ performance through tensile tests on specimens produced by a multimaterial printer. We showcase GenAI’s role in structural augmentation that can yield rational bioinspired designs, complemented by an educational web page with interactive games for public access.}
}
@article{LENG2026103076,
title = {AIGC-empowered smart manufacturing: Prospects and challenges},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {97},
pages = {103076},
year = {2026},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.103076},
url = {https://www.sciencedirect.com/science/article/pii/S0736584525001309},
author = {Jiewu Leng and Keyou Zheng and Rongjie Li and Chong Chen and Baicun Wang and Qiang Liu and Xin Chen and Weiming Shen},
keywords = {Artificial intelligence generated content, Generative artificial intelligence, Smart manufacturing, GenAI Agent, Industry 5.0},
abstract = {Generative AI (GenAI), the technology behind Artificial Intelligence Generated Content (AIGC), has emerged as a transformative technology in smart manufacturing. However, its full potential and integration within manufacturing processes remain unexplored. This paper presents a comprehensive framework that aligns a GenAI-centered approach with Product Lifecycle Management (PLM), systematically examining the AIGC landscape and its applications across various manufacturing phases. To ensure accuracy and relevance, a human-in-the-loop pipeline is employed to curate and analyze cutting-edge research. Key contributions of this study include: 1) a holistic perspective on AIGC-empowered smart manufacturing, 2) an in-depth analysis of the current technological landscape, and 3) the identification of critical research challenges and future directions. Additionally, the paper considers Industry 5.0 principles, emphasizing human-centricity, sustainability, and resilience. By fostering discussion and collaboration, this review aims to advance innovation and unlock the full potential of AIGC in smart manufacturing.}
}
@article{ROBERTS2024103081,
title = {Artificial intelligence and innovation management: Charting the evolving landscape},
journal = {Technovation},
volume = {136},
pages = {103081},
year = {2024},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2024.103081},
url = {https://www.sciencedirect.com/science/article/pii/S0166497224001317},
author = {Deborah L. Roberts and Marina Candi},
keywords = {Artificial intelligence, Generative artificial intelligence, Innovation management, Innovation process},
abstract = {The excitement surrounding Artificial Intelligence (AI) is palpable. It is rapidly gaining prevalence in academia, business, and personal use. In particular, the emergence of generative AI, exemplified by large language models such as ChatGPT, has been marked by substantial media attention, discourse, and hype. Like most, if not all, aspects of business, innovation processes have been impacted. However, little is known about the degree of impact or the benefits that might be gained. To cut through the hype and understand the use of AI in innovation processes in businesses today, a large-scale survey amongst innovation managers in the USA was conducted, followed by interviews. The findings indicate that the use of AI in innovation processes is high and widespread, with AI being used for more than half of the surveyed firms' innovation projects. Furthermore, AI is used more in the development stage of the innovation process than in the idea or commercialization stages, which counters much of the existing discourse, which focuses on the idea stage. We uncover interesting differences by comparing the use and impact of generative AI with that of more traditional AI. Among these is a significant difference in expected benefits in making employees’ jobs more fulfilling — managers believe generative AI is more likely to confer this benefit than traditional AI. This paper offers two valuable contributions. First, it enriches the evolving dialogue at the intersection of AI and innovation management by offering much-needed empirical evidence about practical applications. Second, it provides timely managerial implications by examining relationships between the use of AI and innovation performance and understanding the benefits that AI can confer in the innovation process.}
}
@article{BLEASE2024,
title = {Generative Language Models and Open Notes: Exploring the Promise and Limitations},
journal = {JMIR Medical Education},
volume = {10},
year = {2024},
issn = {2369-3762},
doi = {https://doi.org/10.2196/51183},
url = {https://www.sciencedirect.com/science/article/pii/S2369376224000035},
author = {Charlotte Blease and John Torous and Brian McMillan and Maria Hägglund and Kenneth D Mandl},
keywords = {ChatGPT, generative language models, large language models, medical education, Open Notes, online record access, patient-centered care, empathy, language model, online record access, documentation, communication tool, clinical documentation},
abstract = {Patients’ online record access (ORA) is growing worldwide. In some countries, including the United States and Sweden, access is advanced with patients obtaining rapid access to their full records on the web including laboratory and test results, lists of prescribed medications, vaccinations, and even the very narrative reports written by clinicians (the latter, commonly referred to as “open notes”). In the United States, patient’s ORA is also available in a downloadable form for use with other apps. While survey studies have shown that some patients report many benefits from ORA, there remain challenges with implementation around writing clinical documentation that patients may now read. With ORA, the functionality of the record is evolving; it is no longer only an aide memoire for doctors but also a communication tool for patients. Studies suggest that clinicians are changing how they write documentation, inviting worries about accuracy and completeness. Other concerns include work burdens; while few objective studies have examined the impact of ORA on workload, some research suggests that clinicians are spending more time writing notes and answering queries related to patients’ records. Aimed at addressing some of these concerns, clinician and patient education strategies have been proposed. In this viewpoint paper, we explore these approaches and suggest another longer-term strategy: the use of generative artificial intelligence (AI) to support clinicians in documenting narrative summaries that patients will find easier to understand. Applied to narrative clinical documentation, we suggest that such approaches may significantly help preserve the accuracy of notes, strengthen writing clarity and signals of empathy and patient-centered care, and serve as a buffer against documentation work burdens. However, we also consider the current risks associated with existing generative AI. We emphasize that for this innovation to play a key role in ORA, the cocreation of clinical notes will be imperative. We also caution that clinicians will need to be supported in how to work alongside generative AI to optimize its considerable potential.}
}
@article{NISHISAKO2025,
title = {Reducing Hallucinations and Trade-Offs in Responses in Generative AI Chatbots for Cancer Information: Development and Evaluation Study},
journal = {JMIR Cancer},
volume = {11},
year = {2025},
issn = {2369-1999},
doi = {https://doi.org/10.2196/70176},
url = {https://www.sciencedirect.com/science/article/pii/S2369199925001077},
author = {Sota Nishisako and Takahiro Higashi and Fumihiko Wakao},
keywords = {artificial intelligence, AI, generative AI chatbot, generative pretrained transformer, GPT, retrieval-augmented generation, RAG, hallucination, medical information provision, cancer information service},
abstract = {Background
Generative artificial intelligence (AI) is increasingly used to find information. Providing accurate information is essential to support patients with cancer and their families; however, information returned by generative AIs is sometimes wrong. Returning wrong information is called hallucination. Retrieval-augmented generation (RAG), which supplements large language model (LLM) outputs with relevant external sources, has the potential to reduce hallucinations. Although RAG has been proposed as a promising technique, its real-world performance in public health communication remains underexplored.
Objective
This study aimed to examine cancer information returned by generative AIs with RAG using cancer-specific information sources and general internet searches to determine whether using RAG with reliable information sources reduces the hallucination rates of generative AI chatbots.
Methods
We developed 6 types of chatbots by combining 3 patterns of reference information with 2 versions of LLMs. Thus, GPT-4 and GPT-3.5 chatbots that use cancer information service (CIS) information, Google information, and no reference information (conventional chatbots) were developed. A total of 62 cancer-related questions in Japanese were compiled from public sources. All responses were generated automatically and independently reviewed by 2 experienced clinicians. The reviewers assessed the presence of hallucinations, defined as medically harmful or misinformation. We compared hallucination rates across chatbot types and calculated odds ratios (OR) using generalized linear mixed-effects models. Subgroup analyses were also performed based on whether questions were covered by CIS content.
Results
For the chatbots that used information from CIS, the hallucination rates were 0% for GPT-4 and 6% for GPT-3.5, whereas those for chatbots that used information from Google were 6% and 10% for GPT-4 and GPT-3.5, respectively. For questions on information that is not issued by CIS, the hallucination rates for Google-based chatbots were 19% for GPT-4 and 35% for GPT-3.5. The hallucination rates for conventional chatbots were approximately 40%. Using reference data from Google searches generated more hallucinations than using CIS data, with an OR of 9.4 (95% CI 1.2‐17.5, P<.01); the OR for the conventional chatbot was 16.1 (95% CI 3.7‐50.0, P<.001). While conventional chatbots always generated a response, the RAG-based chatbots sometimes declined to answer when information was lacking. The conventional chatbots responded to all questions, but the response rate decreased (36% to 81%) for RAG-based chatbots. For questions on information not covered by CIS, the CIS chatbots did not respond, while the Google chatbots generated responses in 52% of the cases for GPT-4 and 71% for GPT-3.5.
Conclusions
Using RAG with reliable information sources significantly reduces the hallucination rate of generative AI chatbots and increases the ability to admit lack of information, making them more suitable for general use, where users need to be provided with accurate information.}
}
@article{RODRIGUEZSANCHEZ2025103036,
title = {Generative AI as a source of information on environmental Problems: Understanding its influence on Generation Z},
journal = {Technology in Society},
volume = {83},
pages = {103036},
year = {2025},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.103036},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2500226X},
author = {Carla Rodriguez-Sanchez and Franco Manuel Sancho-Esper and Luis Vicente Casaló and Manuela López},
keywords = {Generative AI, ChatGPT, Environmental attitudes, Information usefulness, Information credibility, Technological innovativeness},
abstract = {Generative artificial intelligence (GenAI) is rapidly emerging as a transformative tool in digital information dissemination. However, its influence on environmental communication problems remains underexplored. This research examines whether exposure to information on an environmental problem provided by GenAI (namely ChatGPT) influences individuals' pro-environmental outcomes in terms of awareness of environmental consequences, moral obligation, and attitudes toward that problem. It addresses the research gap on GenAI's effectiveness compared to traditional sources (digital newspapers) in shaping environmental perceptions, as well as the underlying mechanisms behind this effectiveness. Two experimental studies were conducted with samples of Generation Z users given this demographic's high level of engagement with AI tools. Study 1 examined changes in pro-environmental outcomes when participants were exposed to ChatGPT-generated messages about water scarcity or messages in a digital newspaper. Study 2 focused on the underlying mechanisms of ChatGPT's influence. It examined how perceived usefulness of information mediates this influence and how individual characteristics (psychological distance to the problem and technological innovativeness) moderate this process. Exposure to messages about water scarcity significantly enhanced participants' awareness of consequences, moral obligation, and attitudes toward the problem, regardless of the source. In Study 2, emphasizing problem severity enhanced perceived usefulness, a key factor in shaping pro-environmental outcomes. However, this effect varied based on individual characteristics, with moderation by psychological distance and technological innovativeness. These findings contribute to the growing discussion about the role of GenAI in environmental communication. The paper discusses the potential implications of GenAI for fostering pro-environmental attitudes and behaviors. It emphasizes the need to consider audience-specific factors to maximize the effectiveness of GenAI.}
}
@article{ZHANG2025103046,
title = {Horizontal multi-party data publishing via discriminator regularization and adaptive noise under differential privacy},
journal = {Information Fusion},
volume = {120},
pages = {103046},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103046},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001198},
author = {Pengfei Zhang and Xiang Fang and Zhikun Zhang and Xianjin Fang and Yining Liu and Ji Zhang},
keywords = {Generative Artificial Intelligence, Generative adversarial network, Differential privacy, Multi-party data publishing, Information fusion},
abstract = {With the rapid proliferation of data collection and storage technologies, the growing demand for horizontal multi-party data publishing has created an urgent need for robust privacy-preserving mechanisms that can effectively handle sensitive distributed data across multiple organizations. While existing approaches attempt to address this challenge, they often fail to balance privacy protection with data utility, struggle to achieve effective information fusion across heterogeneous data distributions, and incur significant computational overhead. In this paper, we introduce the NATION approach, an innovative GAN-based framework that advances multi-party data publishing through sophisticated information fusion techniques while maintaining stringent differential privacy guarantees and computational efficiency. In NATION, we modify the traditional GAN architecture through a distributed design where multiple discriminators are strategically allocated across parties while centralizing the generator at a semi-trusted server, enabling seamless fusion of distributed knowledge with minimal computational cost. Building on this foundation, we introduce two key technical innovations: an iterative-aware adaptive noise IAN method that dynamically optimizes noise injection based on training convergence, and a global-aware discriminator regularization GDR method that leverages Bregman Divergence to enhance inter-discriminator information exchange while ensuring model stability. Through comprehensive theoretical analysis and extensive experimental evaluation on real-world datasets, we demonstrate that NATION consistently outperforms state-of-the-art approaches by up to 7% in accuracy while providing provable privacy guarantees, which makes a significant advancement in secure GAN-based information fusion for privacy-sensitive applications.}
}
@article{NAMOUN2024671,
title = {Predicting the usability of mobile applications using AI tools: the rise of large user interface models, opportunities, and challenges},
journal = {Procedia Computer Science},
volume = {238},
pages = {671-682},
year = {2024},
note = {The 15th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) / The 7th International Conference on Emerging Data and Industry 4.0 (EDI40), April 23-25, 2024, Hasselt University, Belgium},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.076},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924013127},
author = {Abdallah Namoun and Ahmed Alrehaili and Zaib Un Nisa and Hani Almoamari and Ali Tufail},
keywords = {generative artificial intelligence, LLM, generative UI design, large UI models, mobile apps, usability testing, usability attributes},
abstract = {This article proposes the so-called large user interface models (LUIMs) to enable the generation of user interfaces and prediction of usability using artificial intelligence in the context of mobile applications. To this end, we synergized an integrated framework for the effective testing of the usability of mobile applications following a selective review of the most influential models of mobile usability testing. Next, we identified and analysed 13 recent AI tools that generate user interfaces for mobile apps, and systematically tested these tools to identify their AI capabilities. Our striking findings demonstrate that current generative UI tools fail to address mobile usability attributes, such as efficiency, learnability, effectiveness, satisfaction, and memorability. Our large UI models’ architecture proposes to leverage the capabilities of large language models, large vision models, and large code models to overcome the challenges of AI-driven UI/UX design and front-end implementations. This fascinating UI eco-system must be augmented with sufficient UI data and multi-sensory input regarding user behaviour to train the models. We anticipate LUIMs to create ample opportunities, like expedited frontend software development, enhanced personalised user experience, and wider accessibility of smart technologies. However, the research challenges hindering the UI generation and usability prediction of mobile apps include the seamless integration of complex generative AI models, semantic understanding of non-uniform visual designs, scarcity of UX datasets, and modelling of realistic user interactions.}
}
@article{URBAN2024105031,
title = {ChatGPT improves creative problem-solving performance in university students: An experimental study},
journal = {Computers & Education},
volume = {215},
pages = {105031},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105031},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524000459},
author = {Marek Urban and Filip Děchtěrenko and Jiří Lukavský and Veronika Hrabalová and Filip Svacha and Cyril Brom and Kamila Urban},
keywords = {Generative AI, ChatGPT, Creativity, Metacognitive monitoring, Metacognitive experiences, Ill-defined problem-solving task},
abstract = {University students often employ generative artificial intelligence tools such as ChatGPT in resolution of ill-defined problem-solving tasks. However, the experimental evidence about effects of ChatGPT on complex problem-solving performance is still missing. In this preregistered experiment, the impact of ChatGPT on performance in a complex creative problem-solving task was investigated in 77 university students solving a task with ChatGPT in comparison to 68 students solving a task without it. ChatGPT use significantly improved self-efficacy for task resolution (d = 0.65) and enhanced the quality (d = 0.69), elaboration (d = 0.61), and originality (d = 0.55) of solutions. Moreover, participants with ChatGPT assistance perceived task as easier (d = 0.56) and requiring less mental effort (d = 0.58). However, use of ChatGPT did not make task resolution more interesting (d = 0.08), and the impact of ChatGPT on metacognitive monitoring accuracy was unclear. Although there were no significant differences in absolute accuracy between students solving the task with and without the assistance of ChatGPT, the absence of correlation between self-evaluation judgments and performance suggests that participants struggled to calibrate their self-evaluations when using ChatGPT. Notably, the perceived usefulness of ChatGPT appeared to inform self-evaluation judgments, resulting in higher inaccuracy. The implications for hybrid human-AI regulation (HHAIR) theory are discussed. To regulate effectively, students using AI tools should focus on valid metacognitive cues instead of the perceived ease of ChatGPT-assisted problem-solving.}
}
@article{YU2025107802,
title = {Measuring the quality of generative AI systems: Mapping metrics to quality characteristics — Snowballing literature review},
journal = {Information and Software Technology},
volume = {186},
pages = {107802},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107802},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925001417},
author = {Liang Yu and Emil Alégroth and Panagiota Chatzipetrou and Tony Gorschek},
keywords = {Generative AI, GenAI, Large language model, LLM, Quality characteristics, Metric, Evaluation},
abstract = {Context
Generative Artificial Intelligence (GenAI) and the use of Large Language Models (LLMs) have revolutionized tasks that previously required significant human effort, which has attracted considerable interest from industry stakeholders. This growing interest has accelerated the integration of AI models into various industrial applications. However, the model integration introduces challenges to product quality, as conventional quality measuring methods may fail to assess GenAI systems. Consequently, evaluation techniques for GenAI systems need to be adapted and refined. Examining the current state and applicability of evaluation techniques for the GenAI system outputs is essential.
Objective
This study aims to explore the current metrics, methods, and processes for assessing the outputs of GenAI systems and the potential of risky outputs.
Method
We performed a snowballing literature review to identify metrics, evaluation methods, and evaluation processes from 43 selected papers.
Results
We identified 28 metrics and mapped these metrics to four quality characteristics defined by the ISO/IEC 25023 standard for software systems. Additionally, we discovered three types of evaluation methods to measure the quality of system outputs and a three-step process to assess faulty system outputs. Based on these insights, we suggested a five-step framework for measuring system quality while utilizing GenAI models.
Conclusion
Our findings present a mapping that visualizes candidate metrics to be selected for measuring quality characteristics of GenAI systems, accompanied by step-by-step processes to assist practitioners in conducting quality assessments.}
}
@article{HWANG2025102266,
title = {Who owns AI-generated artwork? Revisiting the work of generative AI based on human-AI co-creation},
journal = {Telematics and Informatics},
volume = {98},
pages = {102266},
year = {2025},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2025.102266},
url = {https://www.sciencedirect.com/science/article/pii/S0736585325000280},
author = {Yohan Hwang and Dongkwang Shin and Jang Ho Lee},
keywords = {Generative AI, Text-to-image creator, Ownership, Copyright, Human-computer interaction},
abstract = {Generative Artificial Intelligence (AI) tools have become increasingly common in educational contexts. However, questions remain regarding the ownership and copyright of intellectual property generated through human-AI collaboration. The present study explored Korean college students’ perceptions of ownership and copyright over images generated through the iterative process of refining prompts within AI image generators. Participants engaged in the image generation project of visualizing the meaning of target words using ChatGPT to generate prompts for the Bing Image Creator. Post-project survey and reflection papers were administered to assess their emotions, sense of ownership, and copyright views related to AI-generated images through the prompts. Sentiment analysis revealed the complexity of participants’ attitudes toward AI and its creation. Views on copyright varied from individual attribution to shared or uncertain ownership. Frequency of prompt use significantly influenced ownership and prompt-based perceptions of copyright. The results of this study provide insight into the intellectual property issues arising from human-AI co-creation, and point to the urgent need for more discussion and balanced views on creative collaboration with AI.}
}
@article{LAU2025,
title = {Exploring the Acceptance and Opportunities of Using a Specific Generative AI Chatbot to Assist Parents in Managing Pediatric Rheumatological Chronic Health Conditions: Mixed Methods Study},
journal = {JMIR Pediatrics and Parenting},
volume = {8},
year = {2025},
issn = {2561-6722},
doi = {https://doi.org/10.2196/70409},
url = {https://www.sciencedirect.com/science/article/pii/S2561672225000604},
author = {Cheryl W Y Lau and Klaudia Kupiec and Polly Livermore},
keywords = {pediatric health care chatbot, technology acceptance, parental attitudes, children and young people's involvement, chronic disease management, AI hesitancy, chronic health condition, artificial intelligence},
abstract = {Background
Health care chatbots can be used to support patients and their families with everyday decision-making. While there is some research on integrating artificial intelligence into pediatric care, no study has focused on the opportunity of implementing a generative artificial intelligence chatbot for pediatric rheumatology. Pediatric rheumatology conditions require intense family input, which can often leave families struggling to navigate disease flares, pain, fatigue, medication side effects and adherence, and support of their child, often when pediatric rheumatology departments are shut. Understanding how we can support families better, without the need for increased personnel, will have implications for the health care systems.
Objective
The study aimed to explore parental and children and young people’s acceptance of chatbot use in a pediatric context, and understand how a chatbot could be specifically used for managing a child’s chronic health condition.
Methods
This study was a mixed methods design, using both a family workshop and a subsequent questionnaire.
Results
In total, 22 participants contributed to the qualitative design using the world café methodology at a workshop, and 47 participants (36 parents and 11 children and young people) completed quantitative data via a questionnaire. Participants expressed their likelihood of using chatbot technology, including ChatGPT, due to its accessibility. However, participants had significantly greater intention (parents: P<.001; children and young people: P=.006) to use a specific chatbot over ChatGPT, due to increased trust, credibility, and specificity in design. Children and young people and parents should be distinguished as 2 user groups in chatbot design, reflecting their specific needs in chatbot features and personalization.
Conclusions
Overall, the study reinforced the need for a specialized and trusted chatbot designed with input from health professionals to assist families in managing complex chronic health conditions to support families in between appointments and complement existing face-to-face care. Future research should evaluate users’ engagement with a functional prototype to investigate its usefulness and explore its implementation into families’ everyday lives. Importantly, the current findings have broader implications for the field of pediatric health care, as similarly tailored chatbot interventions could benefit families who are managing other chronic health conditions.}
}
@article{LI2025,
title = {Exploring the GenAI Literacy of Chinese University Students in EFL Learning},
journal = {International Journal of Computer-Assisted Language Learning and Teaching},
volume = {15},
number = {1},
year = {2025},
issn = {2155-7098},
doi = {https://doi.org/10.4018/IJCALLT.377175},
url = {https://www.sciencedirect.com/science/article/pii/S215570982500009X},
author = {Tongtong Li and Yan Ding},
keywords = {Content Analysis, EFL Students, Focus Groups, GenAI Overdependence, GenAI Underuse, GenAI Usage Reports, Generative AI, Literacy, Process-Oriented Framework},
abstract = {ABSTRACT
Although research on generative artificial intelligence (GenAI) has expanded rapidly, limited attention has been given to students’ GenAI literacy in English as a foreign language learning, particularly from a process-oriented perspective. To address this gap, a process-oriented analytical framework was proposed and applied to examine Chinese university students’ GenAI literacy in English as a foreign language learning. The analysis drew on three datasets from the same cohort of 144 students: focus group transcripts, GenAI usage reports from a video presentation project, and presentation scripts developed with GenAI support. The findings reveal dual challenges in students’ GenAI literacy: a tendency to underutilize advanced functionalities while over-relying on GenAI for basic tasks. Additionally, the results underscore the significant influence of contextual factors in shaping student–GenAI interactions. Theoretical and pedagogical implications of these findings are discussed.}
}
@article{BOUSQUET2025,
title = {Advantages and Inconveniences of a Multi-Agent Large Language Model System to Mitigate Cognitive Biases in Diagnostic Challenges},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/69742},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125000871},
author = {Cedric Bousquet and Divà Beltramin},
keywords = {large language model, multi-agent system, diagnostic errors, cognition, clinical decision-making, cognitive bias, generative artificial intelligence}
}
@article{ALTORFER20251635,
title = {The double-edged sword of generative AI: surpassing an expert or a deceptive “false friend”?},
journal = {The Spine Journal},
volume = {25},
number = {8},
pages = {1635-1643},
year = {2025},
issn = {1529-9430},
doi = {https://doi.org/10.1016/j.spinee.2025.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1529943025001226},
author = {Franziska C.S. Altorfer and Michael J. Kelly and Fedan Avrumova and Varun Rohatgi and Jiaqi Zhu and Christopher M. Bono and Darren R. Lebl},
keywords = {Bard, ChatGPT, Claude, Evidence-based, Gemini, Generative, LLM, NASS, Spine},
abstract = {BACKGROUND CONTEXT
Generative artificial intelligence (AI), ChatGPT being the most popular example, has been extensively assessed for its capability to respond to medical questions, such as queries in spine treatment approaches or technological advances. However, it often lacks scientific foundation or fabricates inauthentic references, also known as AI hallucinations.
PURPOSE
To develop an understanding of the scientific basis of generative AI tools by studying the authenticity of references and reliability in comparison to the alignment of responses of evidence-based guidelines.
STUDY DESIGN
Comparative study.
METHODS
Thirty-three previously published North American Spine Society (NASS) guideline questions were posed as prompts to 2 freely available generative AI tools (Tools I and II). The responses were scored for correctness compared with the published NASS guideline responses using a 5-point “alignment score." Furthermore, all cited references were evaluated for authenticity, source type, year of publication, and inclusion in the scientific guidelines.
RESULTS
Both tools’ responses to guideline questions achieved an overall score of 3.5±1.1, which is considered acceptable to be equivalent to the guideline. Both tools generated 254 references to support their responses, of which 76.0% (n=193) were authentic and 24.0% (n=61) were fabricated. From these, authentic references were: peer-reviewed scientific research papers (147, 76.2%), guidelines (16, 8.3%), educational websites (9, 4.7%), books (9, 4.7%), a government website (1, 0.5%), insurance websites (6, 3.1%) and newspaper websites (5, 2.6%). Claude referenced significantly more authentic peer-reviewed scientific papers (Claude: n=111, 91.0%; Gemini: n=36, 50.7%; p<.001). The year of publication amongst all references ranged from 1988-2023, with significantly older references provided by Claude (Claude: 2008±6; Gemini: 2014±6; p<.001). Lastly, significantly more references provided by Claude were also referenced in the published NASS guidelines (Claude: n=27, 24.3%; Gemini: n=1, 2.8%; p=.04).
CONCLUSIONS
Both generative AI tools provided responses that had acceptable alignment with NASS evidence-based guideline recommendations and offered references, though nearly a quarter of the references were inauthentic or nonscientific sources. This deficiency of legitimate scientific references does not meet standards for clinical implementation. Considering this limitation, caution should be exercised when applying the output of generative AI tools to clinical applications.}
}
@article{SATTELMAIER2025101838,
title = {Be AIware! An AI Competency Model for K-12 Education},
journal = {Social Sciences & Humanities Open},
volume = {12},
pages = {101838},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101838},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125005662},
author = {Lana Sattelmaier and Jan Pawlowski},
keywords = {21st century abilities, AI Literacy, K-12 education, AI competencies, Proficiency levels},
abstract = {The rapid emergence of generative artificial intelligence (GAI) is reshaping educational contexts, creating an urgent need for K–12 systems to adapt. Teachers, in particular, face increasing demands to respond to AI-driven transformations in learning environments yet lack structured guidance on which competencies matter. This paper presents the AIware Competency Model, a comprehensive framework of 98 AI-related competencies designed for K–12 education, including emerging aspects of GAI. The model was developed using an Action Design Research (ADR) approach, structured across four iterative stages: (1) a systematic literature review and analysis of 14 existing AI competency models to identify conceptual gaps; (2) semi-structured interviews with AI experts and educators (n = 5) to inform initial model development; (3) three rounds of focus groups involving teachers, policymakers, and AI professionals (total n = 46) to refine and evaluate the competencies; and (4) a competency ranking process (n = 10) to assess perceived importance across educational roles. The AIware Competency Model addresses current fragmentation in AI education research by offering a unified, empirically grounded framework aligned with practical classroom needs. It enables mapping to existing curriculum standards and supports both teacher preparation and student readiness in a world increasingly shaped by AI. The model contributes to the field by combining theoretical synthesis with practitioner input, offering a robust foundation for future research, curriculum design, and policy development.}
}
@article{YANG2025102560,
title = {Mapping the intersection of heart rate variability and complementary medicine: a two-decade bibliometric study (2005–2024)},
journal = {European Journal of Integrative Medicine},
volume = {79},
pages = {102560},
year = {2025},
issn = {1876-3820},
doi = {https://doi.org/10.1016/j.eujim.2025.102560},
url = {https://www.sciencedirect.com/science/article/pii/S187638202500109X},
author = {Shih-Wei Yang and Chen-Wei Chang and Malcolm Koo},
keywords = {Heart rate variability, Autonomic nervous system, Complementary medicine, Integrative medicine, Bibliometrics},
abstract = {Introduction
Heart Rate Variability (HRV), a noninvasive marker of autonomic nervous system function, has become an increasingly utilized tool in complementary medicine (CM) research for objectively assessing physiological responses to interventions. While specific applications of HRV within individual CM modalities have been reviewed, comprehensive mapping of the broader research landscape remains limited. This bibliometric study aimed to provide a detailed overview of the evolution, key contributors, and thematic development of HRV research across diverse CM interventions between 2005 and 2024.
Methods
Original research articles published between January 1, 2005, and December 31, 2024, were retrieved from the Science Citation Index Expanded of the Web of Science Core Collection. A Boolean search strategy combined HRV-related terms with keywords representing a wide range of CM interventions, while deliberately excluding broad or ambiguous descriptors. The data were analyzed using the Bibliometrix package.
Results
A total of 1007 articles published across 375 journals and authored by 4969 individuals were identified, with publication output showing a steady increase and peaking in 2022. China (23.5%) and the United States (19.1%) emerged as the most prolific contributors, although United States publications exhibited a higher average citation rate. Leading institutions included Kyung Hee University (South Korea) and China Medical University (Taiwan). Evidence-Based Complementary and Alternative Medicine was the most frequent publishing journal. HRV and CM research has increasingly appeared in higher-ranked journals over time (p < 0.001). Keyword co-occurrence analysis positioned HRV as a central term, connecting clusters related to mind-body practices, psychophysiological constructs, music therapy, and electroacupuncture. Thematic evolution analysis revealed recent trends toward greater methodological rigor, increased emphasis on patient-centered outcomes, diversification of CM interventions, and incorporation of emerging digital health technologies.
Conclusion
This bibliometric analysis shows HRV research in CM as an evolving field positioned at the intersection of traditional practices, contemporary physiological science, and technological innovation. Recent trends indicate a shift toward increased methodological rigor, greater emphasis on patient-centered outcomes, and integration of digital technologies. As the field advances, novel technologies such as generative artificial intelligence offer promise for deepening physiological insights and enhancing the clinical relevance of CM applications within mainstream healthcare.}
}
@article{SKULMOWSKI2023100023,
title = {Ethical issues of educational virtual reality},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100023},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100023},
url = {https://www.sciencedirect.com/science/article/pii/S294967802300017X},
author = {Alexander Skulmowski},
keywords = {Virtual reality, Education, Ethics, Realism, Autonomy, Privacy},
abstract = {In response to the high demand for digital learning as a surrogate for physical experiences, virtual reality (VR) is positioning itself as a tool for creating educational virtual experiences. VR technology faces a number of ethical issues, including a reduction of users’ autonomy, health problems, and privacy concerns. The use of VR and realism in education can turn out to be a double-edged sword. While realistic visualizations can promote learning for some content domains, they can hinder comprehension in others. Furthermore, the effects of realism on learning also depend on learners’ spatial abilities. Letting young children and teenagers engage in virtual educational experiences can expose them to manipulation, could lead to health issues, and may infringe on their privacy. In short, realism and virtual experiences may severely limit learners’ autonomy in a number of ways. Based on a review of the literature and considerations of emerging technologies such as generative artificial intelligence, this paper presents guidelines for the ethically sound utilization of VR and realism. By applying findings and conclusions established in the context of research on the ethics of VR to the educational utilization of this technology, I develop several suggestions that may help to avoid negative consequences of educational VR. These suggestions include the utilization of spatial ability testing, requiring virtual experiences to offer alternative paths to prevent manipulation, as well as using algorithms that deidentify the highly detailed developmental profiles that can be generated through educational VR use.}
}
@article{LIU2025112278,
title = {Agent design pattern catalogue: A collection of architectural patterns for foundation model based agents},
journal = {Journal of Systems and Software},
volume = {220},
pages = {112278},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112278},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003224},
author = {Yue Liu and Sin Kit Lo and Qinghua Lu and Liming Zhu and Dehai Zhao and Xiwei Xu and Stefan Harrer and Jon Whittle},
keywords = {Agent, Foundation model, Large language model, Pattern, Software engineering, Responsible AI},
abstract = {Foundation model-enabled generative artificial intelligence facilitates the development and implementation of agents, which can leverage distinguished reasoning and language processing capabilities to takes a proactive, autonomous role to pursue users’ goals. Nevertheless, there is a lack of systematic knowledge to guide practitioners in designing the agents considering challenges of goal-seeking (including generating instrumental goals and plans), such as hallucinations inherent in foundation models, explainability of reasoning process, complex accountability, etc. To address this issue, we have performed a systematic literature review to understand the state-of-the-art foundation model-based agents and the broader ecosystem. In this paper, we present a pattern catalogue consisting of 18 architectural patterns with analyses of the context, forces, and trade-offs as the outcomes from the previous literature review. We propose a decision model for selecting the patterns. The proposed catalogue can provide holistic guidance for the effective use of patterns, and support the architecture design of foundation model-based agents by facilitating goal-seeking and plan generation.}
}
@article{LEVKOVICH2025104970,
title = {Attributional patterns toward students with and without learning disabilities: Artificial intelligence models vs. trainee teachers},
journal = {Research in Developmental Disabilities},
volume = {160},
pages = {104970},
year = {2025},
issn = {0891-4222},
doi = {https://doi.org/10.1016/j.ridd.2025.104970},
url = {https://www.sciencedirect.com/science/article/pii/S089142222500054X},
author = {Inbar Levkovich and Eyal Rabin and Rania Hussein Farraj and Zohar Elyoseph},
keywords = {Generative artificial intelligence, Trainee teachers, Attribution, Learning disabilities, Expectations, Cultural differences},
abstract = {This study explored differences in the attributional patterns of four advanced artificial intelligence (AI) Large Language Models (LLMs): ChatGPT3.5, ChatGPT4, Claude, and Gemini) by focusing on feedback, frustration, sympathy, and expectations of future failure among students with and without learning disabilities (LD). These findings were compared with responses from a sample of Australian and Chinese trainee teachers, comprising individuals nearing qualification with varied demographic and educational backgrounds. Eight vignettes depicting students with varying abilities and efforts were evaluated by the LLMs ten times each, resulting in 320 evaluations, with trainee teachers providing comparable ratings. For LD students, the LLMs exhibited lower frustration and higher sympathy than trainee teachers, while for non-LD students, LLMs similarly showed lower frustration, with ChatGPT3.5 aligning closely with Chinese teachers and ChatGPT4 demonstrating more sympathy than both teacher groups. Notably, LLMs expressed lower expectations of future academic failure for both LD and non-LD students compared to trainee teachers. Regarding feedback, the findings reflect ratings of the qualitative nature of feedback LLMs and teachers would provide, rather than actual feedback text. The LLMs, particularly ChatGPT3.5 and Gemini, were rated as providing more negative feedback than trainee teachers, while ChatGPT4 provided more positive ratings for both LD and non-LD students, aligning with Chinese teachers in some cases. These findings suggest that LLMs may promote a positive and inclusive outlook for LD students by exhibiting lower judgmental tendencies and higher optimism. However, their tendency to rate feedback more negatively than trainee teachers highlights the need to recalibrate AI tools to better align with cultural and emotional nuances.}
}
@article{VANTAM2025113526,
title = {How generative AI reshapes construction and built environment: The good, the bad, and the ugly},
journal = {Building and Environment},
volume = {284},
pages = {113526},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2025.113526},
url = {https://www.sciencedirect.com/science/article/pii/S0360132325009990},
author = {Nguyen {Van Tam}},
keywords = {Generative AI, Construction, Built environment, The good, The bad, The ugly},
abstract = {Generative Artificial Intelligence (GenAI) is poised to fundamentally transform the Construction and Built Environment (CBE) industry by offering innovative solutions to longstanding challenges. However, like any disruptive innovation, the adoption of GenAI presents a dual-edged sword, offering immense benefits while simultaneously posing considerable difficulties and potential pitfalls. This paper aims to illuminate this complex interplay by systematically uncovering the “Good,” the “Bad,” and the “Ugly” aspects of GenAI adoption within the CBE sector. This discussion provides a valuable foundation for understanding GenAI’s multifaceted impact, thereby offering valuable insights for future research and practical implementation.}
}
@article{GONG2026103733,
title = {Beyond embedding-mapping: Social network alignment via generative information fusion and LLM-guided iterative mechanism},
journal = {Information Fusion},
volume = {127},
pages = {103733},
year = {2026},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103733},
url = {https://www.sciencedirect.com/science/article/pii/S156625352500795X},
author = {Jibing Gong and Jiquan Peng and Wei Wang and Wei Zhou and Chaozhuo Li and Philip S. Yu},
keywords = {Social network alignment, Large language model, Information fusion, Graph learning, Generative artificial intelligence},
abstract = {Social Network Alignment (SNA) aims to identify and match user accounts belonging to the same real-world individual across multiple social platforms, which has garnered growing research interest. Existing methods typically encode textual and structural information into a latent space and learn a mapping function from annotated user alignments to accomplish SNA. However, the inherent sparsity and noise in social data limit these models’ ability to fully capture user characteristics. Moreover, direct alignment based on latent space often overlooks critical details from the original information, reducing both alignment quality and interpretability. To address these limitations, we propose LLM-SNA, a novel framework that integrates generative information fusion and LLM-guided iterative mechanism. The generative information fusion leverages LLMs to transform sparse user attributes, microblogs, and neighbor descriptions into enriched, comprehensive user profiles. We further perform intra-network and inter-network graph learning on enriched user profiles to incorporate structural information. To balance accuracy and efficiency, the LLM-guided iterative mechanism first applies a coarse filter based on embedding similarities to collect potential alignment candidates. The LLM then evaluates these candidates by reasoning over their original textual information. If the LLM deems the candidates misaligned, the candidate set is expanded until confident matches emerge. Comprehensive experiments on three widely used datasets demonstrate the advantages of LLM-SNA over state-of-the-art baseline methods and highlight the potential of LLMs for SNA tasks.}
}
@article{GOLDEN2024,
title = {Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial},
journal = {JMIR Formative Research},
volume = {8},
year = {2024},
issn = {2561-326X},
doi = {https://doi.org/10.2196/62963},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X24005596},
author = {Ashleigh Golden and Elias Aboujaoude},
keywords = {artificial intelligence, ChatGPT, generative artificial intelligence, generative AI, large language model, chatbots, machine learning, digital health, telemedicine, psychotherapy, obsessive-compulsive disorder},
abstract = {As artificial intelligence (AI) technologies occupy a bigger role in psychiatric and psychological care and become the object of increased research attention, industry investment, and public scrutiny, tools for evaluating their clinical, ethical, and user-centricity standards have become essential. In this paper, we first review the history of rating systems used to evaluate AI mental health interventions. We then describe the recently introduced Framework for AI Tool Assessment in Mental Health (FAITA-Mental Health), whose scoring system allows users to grade AI mental health platforms on key domains, including credibility, user experience, crisis management, user agency, health equity, and transparency. Finally, we demonstrate the use of FAITA-Mental Health scale by systematically applying it to OCD Coach, a generative AI tool readily available on the ChatGPT store and designed to help manage the symptoms of obsessive-compulsive disorder. The results offer insights into the utility and limitations of FAITA-Mental Health when applied to “real-world” generative AI platforms in the mental health space, suggesting that the framework effectively identifies key strengths and gaps in AI-driven mental health tools, particularly in areas such as credibility, user experience, and acute crisis management. The results also highlight the need for stringent standards to guide AI integration into mental health care in a manner that is not only effective but also safe and protective of the users’ rights and welfare.}
}
@article{CHANGALIMA2024101063,
title = {Social influence and information quality on Generative AI use among business students},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101063},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101063},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001344},
author = {Ismail Abdi Changalima and David Amani and Ismail Juma Ismail},
keywords = {Artificial intelligence, ChatGPT, Social influence, Information quality, Behavioural intention, Multigroup analysis},
abstract = {Despite the increasing utilisation of generative artificial intelligence (AI) in educational settings, its influence on shaping students’ behaviour remains relatively under-researched. This study employs PLS-SEM to explore the relationships between social influence, information quality, and behavioural intentions regarding ChatGPT usage among business students. Drawing from data collected from 477 business students, the study unveils that both social influence and information quality significantly impact behavioural intentions. Moreover, information quality strengthens the influence of social influence on behavioural intentions. The multigroup analysis reveals that the effects of social influence and information quality on behavioural intentions differ between females and males. However, the moderating effect of information quality does not differ significantly between them. Furthermore, the effects observed in all hypothesised relationships do not differ significantly between first-year and second-year students. By empirically validating the proposed model for behavioural intentions regarding ChatGPT usage and identifying statistical differences among males and females, as well as between first-year and second-year students, this study contributes to filling existing knowledge gaps. Furthermore, the study offers potential avenues for future research and serves as a valuable resource for academics, professionals, and policymakers interested in understanding students' engagement with generative AI.}
}
@article{GUO2023329,
title = {AIGC challenges and opportunities related to public safety: A case study of ChatGPT},
journal = {Journal of Safety Science and Resilience},
volume = {4},
number = {4},
pages = {329-339},
year = {2023},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666449623000397},
author = {Danhuai Guo and Huixuan Chen and Ruoling Wu and Yangang Wang},
keywords = {Generative artificial intelligence， Artificial intelligence generated content, ChatGPT, Public safety, Strong artificial intelligence},
abstract = {Artificial intelligence generated content (AIGC) is a production method based on artificial intelligence (AI) technology that finds rules through data and automatically generates content. In contrast to computational intelligence, generative AI, as exemplified by ChatGPT, exhibits characteristics that increasingly resemble human-level comprehension and creation processes. This paper provides a detailed technical framework and history of ChatGPT, followed by an examination of the challenges posed to political security, military security, economic security, cultural security, social security, ethical security, legal security, machine escape problems, and information leakage. Finally, this paper discusses the potential opportunities that AIGC presents in the realms of politics, military, cybersecurity, society, and public safety education.}
}
@article{LEE2024100283,
title = {Teachers' and students' perceptions of AI-generated concept explanations: Implications for integrating generative AI in computer science education},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100283},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100283},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000869},
author = {Soohwan Lee and Ki-Sang Song},
keywords = {Generative artificial intelligence(GAI), Elementary education, Concept explanations, Computer science education, Perceptual differences},
abstract = {The educational application of Generative AI (GAI) has garnered significant interest, sparking discussions about the pedagogical value of GAI-generated content. This study investigates the perceived effectiveness of concept explanations produced by GAI compared to those created by human teachers, focusing on programming concepts of sequence, selection, and iteration. The research also explores teachers' and students' ability to discern the source of these explanations. Participants included 11 teachers and 70 sixth-grade students who were presented with concept explanations created or generated by teachers and ChatGPT. They were asked to evaluate the helpfulness of the explanations and identify their source. Results indicated that teachers found GAI-generated explanations more helpful for sequence and selection concepts, while preferring teacher-created explanations for iteration (χ2(2, N = 11) = 10.062, p = .007, ω = .595). In contrast, students showed varying abilities to distinguish between AI-generated and teacher-created explanations across concepts, with significant differences observed (χ2(2, N = 70) = 22.127, p < .001, ω = .399). Notably, students demonstrated difficulty in identifying the source of explanations for the iteration concept (χ2(1, N = 70) = 8.45, p = .004, φ = .348). Qualitative analysis of open-ended responses revealed that teachers and students employed similar criteria for evaluating explanations but differed in their ability to discern the source. Teachers focused on pedagogical effectiveness, while students prioritized relatability and clarity. The findings highlight the importance of considering both teachers' and students' perspectives when integrating GAI into computer science education. The study proposes strategies for designing GAI-based explanations that cater to learners' needs and emphasizes the necessity of explicit AI literacy instruction. Limitations and future research directions are discussed, underlining the need for larger-scale studies and experimental designs that assess the impact of GAI on actual learning outcomes.}
}
@article{ATKINSON2025103129,
title = {AI-pocalypse now: Automating the systematic literature review with SPARK (Systematic processing and automated review Kit) – gathering, organising, filtering, and scaffolding.},
journal = {MethodsX},
volume = {14},
pages = {103129},
year = {2025},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2024.103129},
url = {https://www.sciencedirect.com/science/article/pii/S2215016124005806},
author = {Cameron Frederick Atkinson},
keywords = {Systematic literature review, Automation, Scopus, Web of science, Google, LDA Topic Modelling, Python},
abstract = {Researchers today face significant challenges reshaping the landscape of academic, government, and industry research due to the exponential growth of global research outputs and the advent of Generative Artificial Intelligence (GenAI). The annual increase in published works has made it difficult for traditional literature review and data analysis methods to keep pace, often rendering reviews outdated by the time of publication. In response, this methods article introduces a suite of new tools designed to automate a number of stages for systematic literature reviews. Designated SPARK (Systematic Processing and Automated Review Kit), the new computational-based approaches presented in this article automate the collection, organisation, and filtering of journal articles, alongside a data extraction scaffolding technique, for use in a systematic literature review on trauma-informed policing. As global research outputs rise, so does the need for automated methods. This paper highlights how these methods can enhance research efficiency and impact.•Hard-coded tools can be utilised to automate research.•Hard-coded tools do not carry the dangers of ‘hallucinations’ that GenAI infused tools may.•Hard-coded automation tools allow researchers to keep up to date with contemporary research outputs while maintaining a high level of control in the research process.}
}
@article{ORRU2025102086,
title = {Large language models and psychiatry},
journal = {International Journal of Law and Psychiatry},
volume = {101},
pages = {102086},
year = {2025},
issn = {0160-2527},
doi = {https://doi.org/10.1016/j.ijlp.2025.102086},
url = {https://www.sciencedirect.com/science/article/pii/S0160252725000196},
author = {Graziella Orrù and Giulia Melis and Giuseppe Sartori},
keywords = {Large language models, Psychiatry, Reasoning, Intelligence},
abstract = {Integrating Generative Artificial Intelligence and Large Language Models (LLMs) such as GPT-4 is transforming clinical medicine and cognitive psychology. These models exhibit remarkable capabilities in understanding and generating human-like language, which can enhance various aspects of healthcare, including clinical decision-making and psychological counseling. LLMs, trained on vast datasets, function by predicting the next word in a sequence, endowing them with extensive knowledge and reasoning abilities. Their adaptability allows them to perform a wide range of language-related tasks, significantly contributing to advancements in cognitive psychology and psychiatry. These models demonstrate proficiency in tasks such as analogical reasoning, metaphor comprehension, and problem-solving, often achieving performance comparable to neurotypical humans. Despite their impressive capabilities, LLMs still exhibit limitations in causal reasoning and complex planning. However, their continuous improvement, exemplified by the enhanced performance of GPT-4 over its predecessors, suggests a trajectory towards overcoming these challenges. The ongoing debate about the “intelligence” of LLMs revolves around their ability to mimic human-like reasoning and understanding, a focal point of contemporary research. This paper explores the cognitive abilities of LLMs, comparing them with human cognitive processes and examining their performance on various psychological tests. It highlights the emergent properties of LLMs, their potential to transform cognitive psychology, and the different applications of LLMs in psychiatry, highlighting the limitations, the ethical considerations, and the importance of scaling and fine-tuning these models to enhance their capabilities. We also explore the parallels between LLMs and human error patterns, underscoring the significance of using LLMs as models for human cognition. Overall, this paper provides substantial evidence supporting the role of LLMs in reviving associationism as a viable framework for understanding human cognition while acknowledging the current limitations and the need for further research to fully realize their potential.}
}
@article{GONZALEZ2024941,
title = {ChatGPT: What Every Pediatric Surgeon Should Know About Its Potential Uses and Pitfalls},
journal = {Journal of Pediatric Surgery},
volume = {59},
number = {5},
pages = {941-947},
year = {2024},
issn = {0022-3468},
doi = {https://doi.org/10.1016/j.jpedsurg.2024.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022346824000101},
author = {Raquel González and Dan Poenaru and Russell Woo and A Francois Trappey and Stewart Carter and David Darcy and Ellen Encisco and Brian Gulack and Doug Miniati and Edzhem Tombash and Eunice Y. Huang},
keywords = {ChatGPT, Artificial intelligence, Natural language processing, Large language models, Pediatric surgery},
abstract = {ChatGPT - currently the most popular generative artificial intelligence system - has been revolutionizing the world and healthcare since its release in November 2022. ChatGPT is a conversational chatbot that uses machine learning algorithms to enhance its replies based on user interactions and is a part of a broader effort to develop natural language processing that can assist people in their daily lives by understanding and responding to human language in a useful and engaging way. Thus far, many potential applications within healthcare have been described, despite its relatively recent release. This manuscript offers the pediatric surgical community a primer on this new technology and discusses some initial observations about its potential uses and pitfalls. Moreover, it introduces the perspectives of medical journals and surgical societies regarding the use of this artificial intelligence chatbot. As ChatGPT and other large language models continue to evolve, it is the responsibility of the pediatric surgery community to stay abreast of these changes and play an active role in safely incorporating them into our field for the benefit of our patients.
Level of Evidence
V.}
}
@article{DECARDINELSON2024108723,
title = {Generative AI and process systems engineering: The next frontier},
journal = {Computers & Chemical Engineering},
volume = {187},
pages = {108723},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108723},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424001418},
author = {Benjamin Decardi-Nelson and Abdulelah S. Alshehri and Akshay Ajagekar and Fengqi You},
keywords = {Generative AI, Process systems engineering, Large language models, Multiscale},
abstract = {This review article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could potentially advance PSE methodologies, providing insights and prospects for each area. Furthermore, the article identifies and discusses potential challenges in fully leveraging GenAI within PSE, including multiscale modeling, data requirements, evaluation metrics and benchmarks, and trust and safety, thereby deepening the discourse on effective GenAI integration into systems analysis, design, optimization, operations, monitoring, and control. This paper provides a guide for future research focused on the applications of emerging GenAI in PSE.}
}
@article{WEBB2025105835,
title = {Advancing Qualitative Analysis in Professional Disaster and Risk Communication: A Comparative Study of an OpenAI ChatGPT 3.5 Model-Enabled Method for Processing Complex Public Discourse},
journal = {International Journal of Disaster Risk Reduction},
pages = {105835},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105835},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925006594},
author = {Margaret Webb and Harman Singh and Rachel Inman and Sweta Baniya and Andrew Katz},
keywords = {social media discourse analysis, natural language processing, GPT-enabled qualitative analysis, crisis communication, COVID-19, climate change, compounding crisis},
abstract = {Crisis managers and risk communicators face increasing challenges analyzing social media discourse during interconnected crises. This paper introduces an end-to-end method for qualitative codebook generation using the ChatGPT-3.5 Generative Artificial Intelligence (GenAI) Large Language Model (LLM) and Generative Pre-trained Transformer (GPT) from OpenAI, comparing a human-in-the-loop GenAI-enabled approach with traditional qualitative coding through a case study of Twitter discourse on COVID-19 and climate change in Virginia (2020-2022). Methods build on prior work establishing qualitative analysis enhanced by Natural Language Processing (NLP) by stacking multiple iterative coding processes with GenAI and humans-in-the-loop to generate qualitative codebooks. A comparative analysis establishes recommendations for assessing the validity of the GPT-enabled generated codebooks. Human validation confirmed substantial concordance (91.7% agreement) with the GPT-enabled process's coding, revealing structural similarities and distinct patterns between the two approaches’ representation of analysis. This research's contribution is methodological, establishing an approach for conducting and assessing GPT-enabled qualitative analysis while mapping relationships between computationally enhanced and traditional qualitative coding approaches. Findings advance risk communication methods by offering empirical guidance on integrating AI-assisted techniques within traditional qualitative research to maintain analytical rigor. Compared to prior NLP-enabled methods for disaster-related social media discourse analysis, this iterative approach mimics aspects of traditional qualitative codebook development, yielding multi-tiered codebook structures that can capture aspects of complex disaster discourse. While findings demonstrate that GenAI can enable analytical efficiency of traditional (human-only) codebook generation through self-assessment and iterative improvement with the help of humans-in-the-loop, they also illuminate areas of coding process where human expertise remains essential.}
}
@article{HUANG2025101336,
title = {Harnessing large multimodal models in pulmonary CT: the generative AI edge in lung cancer diagnostics},
journal = {The Lancet Regional Health - Western Pacific},
volume = {55},
pages = {101336},
year = {2025},
issn = {2666-6065},
doi = {https://doi.org/10.1016/j.lanwpc.2024.101336},
url = {https://www.sciencedirect.com/science/article/pii/S2666606524003304},
author = {Lihaoyun Huang and Junyi Shen and Anqi Lin and Jian Zhang and Peng Luo and Ting Wei},
abstract = {Background
Generative Artificial Intelligence (Gen-AI) has rapidly advanced in multimodal information processing, particularly in medical applications such as the refinement of instruments and interpretation of medical images. However, limited evidence exists on the diagnostic performance of Gen-AI models in tumor recognition, particularly using computed tomography (CT) images. This study aimed to evaluate the diagnostic capabilities of several prevelant Gen-AI models (GPT-4-turbo, Gemini-pro-vision, Claude-3-opus) in the context of lung CT image analysis.
Methods
This retrospective study analyzed chest CT scans from 404 patients with lung conditions with lung neoplasms (n=184) and non-malignancy (n=210). After standardizing CT images, the diagnostic performance and reliability of three Gen-AI (GPT-4-turbo, Gemini-pro-vision, and Claude-3-opus) were assessed using chi-square tests and Receiver Operating Characteristic (ROC) curves across various clinical scenarios. Likert scale scoring and response rate analysis were employed to evaluate internal diagnostic tendencies, while regression analyses were conducted for model optimization.
Findings
In a cueing environment limited to a single CT image, Gemini demonstrated the highest diagnostic accuracy (92.21%), followed by Claude (91.49%), while GPT exhibited the lowest performance (65.22%). As the complexity of the cueing environment increased, all models experienced a decline in diagnostic accuracy. Claude showed a marginal decrease, whereas Gemini's accuracy fluctuated significantly. Under simplified cueing conditions, the performance of all models improved notably (Gemini AUC = 0.76, Claude AUC = 0.69, GPT AUC = 0.73). Feature identification analysis revealed that Claude and GPT excelled in recognizing key features, particularly prioritizing “Morphology/Margins” when diagnosing primary malignancies, with “spiculated” and “irregular” serving as critical indicators. However, in cases of misdiagnosis or missed diagnoses, Gen-AI exhibited significant deviations across multiple feature dimensions—some even completely contradicted the actual findings. Following optimization through Lasso and stepwise regression, the diagnostic performance of the models was significantly enhanced (AUC = 0.896 and AUC = 0.894, respectively).
Interpretation
Gen-AI shows promising potential in pulmonary CT imaging, particularly in simplified diagnostic settings. However, their limitations in processing complex multi-modal information highlight significant challenges for clinical integration. Ongoing efforts to improve the robustness and reliability of these models are crucial for their successful adoption in healthcare.}
}
@article{KIRK2025114984,
title = {The AI-authorship effect: Understanding authenticity, moral disgust, and consumer responses to AI-generated marketing communications},
journal = {Journal of Business Research},
volume = {186},
pages = {114984},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.114984},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324004880},
author = {Colleen P. Kirk and Julian Givi},
keywords = {Artificial intelligence, Generative AI, Moral disgust, Authenticity, ChatGPT, Large language models},
abstract = {Seven preregistered experiments demonstrate that when consumers believe emotional marketing communications are written by an AI (vs. a human), positive word of mouth and customer loyalty are reduced. Drawing from authenticity theory, we show that this “AI-authorship effect” is attenuated for factual (vs. emotional) messages (Study 2); when an AI only edits the communication (Study 3); when a communication is signed directly by an AI (Study 4); and when consumers believe that most marketing communications are written by AI (Study WA1). Importantly, when consumers believe a communication is reused (i.e., not originally written by the sender), the effect is reversed (Study 6). This “AI-authorship effect” is serially mediated by perceived authenticity (Studies 5 and 6) and moral disgust (Studies 1–6 and WA1). These findings are evidenced using both personalized and mass communications, different emotions, businesses and organizational employees, and both hypothetical and behavioral measures.}
}
@article{CUMMINGS2024102827,
title = {Generative AI in first-year writing: An early analysis of affordances, limitations, and a framework for the future},
journal = {Computers and Composition},
volume = {71},
pages = {102827},
year = {2024},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2024.102827},
url = {https://www.sciencedirect.com/science/article/pii/S8755461524000033},
author = {Robert E. Cummings and Stephen M. Monroe and Marc Watkins},
keywords = {Generative artificial intelligence, First-year composition, ChatGPT, Elicit, Fermat, Wordtune, DEER Praxis},
abstract = {Our First-year Writing program began intentional student engagements with generative AI in the fall of 2022. We developed assignments for brainstorming research questions, writing counterarguments, and editing assistance using the AI tools Elicit, Fermat, and Wordtune. Students felt that the tools were helpful for finding ideas to get started with writing, to find sources once they had started writing, and to get help with counterarguments and alternate word choices. But when given the choice to use the assistants or not, most declined. Generative AI at this stage is unreliable, and many students found the tradeoff in reviewing AI suggestions to be too time consuming. And many students expressed a preference for continuing to develop their own voices through writing. Our experience in engaging AI led to the creation of the DEER praxis, which emphasizes defined engagements with AI tools for specific purposes, and generous use of reflection.}
}
@article{DUBEY2024103689,
title = {Benchmarking operations and supply chain management practices using Generative AI: Towards a theoretical framework},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {189},
pages = {103689},
year = {2024},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103689},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524002801},
author = {Rameshwar Dubey and Angappa Gunasekaran and Thanos Papadopoulos},
keywords = {Generative Artificial Intelligence (Gen AI), Artificial Intelligence Supply Chain Management, Benchmarking, Organisational Theories},
abstract = {Generative Artificial Intelligence (Gen AI) is an up-and-coming technological innovation that has the potential to revolutionise businesses and create significant value. Despite garnering excitement from some quarters, there are still people who are sceptical about its benefits and even fearful of its impact, particularly in the supply chain context, where it is not yet fully understood. To help academics and practitioners better understand the practical implications of Gen AI in benchmarking supply chain management practices, we propose a theoretical toolbox. This toolbox draws from ten popular organisational theories and provides a comprehensive framework for evaluating the usefulness of Gen AI. By expanding theoretical boundaries, the toolbox provides a deeper understanding of the practical applications of Gen AI for researchers and practitioners in supply chain management.}
}
@article{ZAFAR2025100802,
title = {Reimagining human creativity and learning in the age of generative AI: A multi-method meta-thematic synthesis},
journal = {Next Research},
volume = {2},
number = {4},
pages = {100802},
year = {2025},
issn = {3050-4759},
doi = {https://doi.org/10.1016/j.nexres.2025.100802},
url = {https://www.sciencedirect.com/science/article/pii/S3050475925006694},
author = {Muhammad Bilal Zafar and Hassnian Ali and Talha Yasin},
keywords = {Generative ai, Human creativity, Structural topic modeling, Co-creation, Prompt literacy, Distributed agency, Cognitive augmentation, Creative labor, Ai ethics, Information systems},
abstract = {The rise of Generative Artificial Intelligence (GenAI) has catalyzed a profound epistemic shift in how creativity is conceptualized, practiced, and structured within socio-technical systems. As GenAI systems increasingly act not merely as facilitators but as co-creators, they blur traditional boundaries between human and machine agency, raising critical questions about originality, authorship, and creative labor. This paper systematically reviews and synthesizes 137 peer-reviewed articles (2023–2025) to map the evolving intersection of GenAI and human creativity, with a specific focus on implications for adaptive and personalized learning environments. Integrating Structural Topic Modeling (STM) with a meta-thematic synthesis, we identify twelve latent topics and five higher-order meta-themes: co-creation in design and aesthetics; GenAI-supported education and future skills; cognitive augmentation and creative reasoning; organizational knowledge work and market narratives; and ethical tensions surrounding copyright, bias, and representation. Our findings reveal that GenAI not only augments but transforms creativity, redistributing agency, reshaping evaluation criteria, and redefining the loci of value creation in socio-technical systems. These transformations carry significant implications for learner engagement, instructional design, and curriculum personalization. We advance five cross-cutting constructs including prompt literacy, distributed agency, ethical tensions, methodological gaps, and governance futures that collectively reframe creativity and learning in the GenAI era. The paper culminates in a conceptual framework and a future-oriented research agenda, offering theoretically generative and practically actionable insights for educators, designers, researchers, and policymakers seeking to harness GenAI for adaptive and personalized education.}
}
@article{SUFFOLETTO2024,
title = {Deceptively Simple yet Profoundly Impactful: Text Messaging Interventions to Support Health},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/58726},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124005235},
author = {Brian Suffoletto},
keywords = {SMS intervention, behavior, intervention, review, text messaging, SMS, interventions, behaviors, behaviour, behaviours, effectiveness, development, impact, narrative review, physical activity, diet, weight loss, mental health, substance use, meta-analysis, chatbot, chatbots, large language model, LLM, large language models, mobile phone},
abstract = {This paper examines the use of text message (SMS) interventions for health-related behavioral support. It first outlines the historical progress in SMS intervention research publications and the variety of funds from US government agencies. A narrative review follows, highlighting the effectiveness of SMS interventions in key health areas, such as physical activity, diet and weight loss, mental health, and substance use, based on published meta-analyses. It then outlines advantages of text messaging compared to other digital modalities, including the real-time capability to collect information and deliver microdoses of intervention support. Crucial design elements are proposed to optimize effectiveness and longitudinal engagement across communication strategies, psychological foundations, and behavior change tactics. We then discuss advanced functionalities, such as the potential for generative artificial intelligence to improve user interaction. Finally, major challenges to implementation are highlighted, including the absence of a dedicated commercial platform, privacy and security concerns with SMS technology, difficulties integrating SMS interventions with medical informatics systems, and concerns about user engagement. Proposed solutions aim to facilitate the broader application and effectiveness of SMS interventions. Our hope is that these insights can assist researchers and practitioners in using SMS interventions to improve health outcomes and reducing disparities.}
}
@article{ZHANG2025124442,
title = {Diffusion-based inpainting approach for multifunctional short-term load forecasting},
journal = {Applied Energy},
volume = {377},
pages = {124442},
year = {2025},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.124442},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924018257},
author = {Luliang Zhang and Zongxi Jiang and Tianyao Ji and Ziming Chen},
keywords = {Short-term load forecasting, Diffusion model, Imputation, Multifunctional forecasting},
abstract = {Abstact
Short-Term Load Forecasting is of great significance for the economic and stable operation of the power system. Against the background of the breakthrough in generative artificial intelligence based on the Diffusion model, the research on relevant load forecasting methods of the latter is still relatively limited. Therefore, this paper refers to many related excellent works, analyzes the commonalities between image generation tasks and load forecasting tasks, and proposes the Diffusion-based Inpainting Forecasting Method (DIFM). DIFM supports multi-variable inputs and can achieve functions such as load sequence generation, quantile forecasting and missing data imputation, making it a flexible and multifunctional method. The feasibility and performance of this method are validated across multiple datasets, with experimental results revealing that DIFM reduces the mean absolute percentage error by 24.61 % and 17.91 % respectively in short-term load forecasting and load imputation tasks compared to the optimal benchmark models.}
}
@article{GARG2025100380,
title = {Enhancing data analysis and programming skills through structured prompt training: The impact of generative AI in engineering education},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100380},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100380},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000207},
author = {Ashish Garg and K. {Nisumba Soodhani} and Ramkumar Rajendran},
keywords = {Applications in subject areas, Post-secondary education, Teaching/learning strategies},
abstract = {The advent of Generative Artificial Intelligence (GenAI) and large language models like LLama, Palm2, GPT, Gemini, and Claude has revolutionized education by generating human-like text and contextually relevant responses. Our research investigates the impact of structured prompt training on students' learning in data analysis and programming. We experimented with 157 first-year engineering students divided into three groups: a control group (internet access, no GenAI), an experimental group 1 (internet and GenAI without prompt training), and an experimental group 2 (internet and GenAI with prompt training). The prompt training session included techniques like few-shot prompting, chain prompting, and the CLEAR framework. We assessed participants' performance in data analysis tasks using Python, with pre-tests and post-tests measuring their skills in programming across three Bloom's taxonomy levels (understanding, application, and analysis). ANOVA on post-test scores showed significant differences among the groups, with G3 (with prompt training) outperforming G2 (without prompt training) and the control group across all three levels, evidenced by higher mean scores (G3: 6.60, G2: 4.94, Control: 4.28), similar pattern observed in task completion also. These results underscore the effectiveness of structured prompt training in enhancing students' data analysis and programming skills. Our study highlights the potential of GenAI and structured prompt training to transform educational practices and suggests future research directions, including integrating prompt engineering within human-AI collaboration.}
}
@article{CHEN2023100184,
title = {Integrating generative AI in knowledge building},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100184},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100184},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000632},
author = {Bodong Chen and Xinran Zhu and Fernando {Díaz del Castillo H.}},
keywords = {Generative AI, ChatGPT, Knowledge building, Human-AI partnership},
abstract = {Generative artificial intelligence (GenAI) is penetrating in various social sectors, motivating a strong need for teaching AI literacy in younger generations. While substantial efforts have been made to teach AI literacy and to use AI to facilitate learning, few studies have provided empirical accounts of students' nuanced processes of using GenAI for learning. In this study, we engaged a group of high school students in leveraging ChatGPT to support their knowledge building efforts. Following the teacher's pedagogical design, students used ChatGPT for a range of distinct purposes. Student interviews showed detailed processes of using ChatGPT for knowledge building and students' emerging AI literacy in multiple dimensions. This study offers practical implications for the integration of GenAI in K-12 education and urges educators to create spaces and scaffolds for students to mindfully engage with GenAI in the classroom.}
}
@article{BALTASALVADOR2026101958,
title = {Evaluating AI-assisted creative ideation: A crossover study in higher education},
journal = {Thinking Skills and Creativity},
volume = {59},
pages = {101958},
year = {2026},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2025.101958},
url = {https://www.sciencedirect.com/science/article/pii/S187118712500207X},
author = {Rosó Baltà-Salvador and Enric Brasó-Vives and Marta Peña},
keywords = {Co-creativity, Engineering education, Generative artificial intelligence, Higher Education, Human-AI Interaction},
abstract = {As artificial intelligence (AI) becomes increasingly integrated into educational contexts, its impact on students’ creative thinking remains unclear. Given the critical role of creativity in engineering and design education, understanding how AI tools shape students’ ideation processes is essential for developing effective pedagogical practices. This study examines the impact of generative AI, specifically ChatGPT, on creative ideation among undergraduate design engineering students. The research was conducted through a randomised crossover experiment, with students alternating between AI-assisted and unaided ideation tasks. A mixed-methods approach combined quantitative analyses of the 728 ideas generated with a qualitative evaluation of students’ interactions with AI. Results show that the use of AI did not reduce fluency, flexibility, or originality, nor did it lead to thematic homogenisation. However, semantic divergence was significantly lower in the AI-assisted condition, suggesting convergence in the way ideas were formulated. Additionally, AI-assisted ideas more frequently resembled existing products and exhibited reduced textual elaboration. A mixed between-within subjects ANOVA revealed that students who began with AI support produced more original and diverse ideas across both tasks, pointing to a lasting effect of early AI use. Qualitative analysis of student-AI interactions revealed important patterns, including predominantly passive and directive use, with limited exploratory or collaborative engagement. These findings provide new insights into human-AI co-creation and highlight the importance of promoting intentional, critical, and pedagogically guided use of generative AI tools in education.}
}
@article{LIN2024103529,
title = {The grass is not always greener: Teacher vs. GPT-assisted written corrective feedback},
journal = {System},
volume = {127},
pages = {103529},
year = {2024},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2024.103529},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X24003117},
author = {Shiming Lin and Peter Crosthwaite},
keywords = {Written corrective feedback, ChatGPT, Generative AI, Automated written corrective feedback, Second language writing},
abstract = {Written Corrective Feedback (WCF) is a crucial pedagogical practice where teachers annotate student writing to correct errors and improve language skills, albeit one that is time-consuming and laborious for large classes or under time constraints. However, the advent of advanced generative artificial intelligence and large language models, specifically ChatGPT, has introduced new possibilities for automating such educational tasks. GPT models with their transformer architecture and self-attention mechanism can perform complex natural language tasks including assisting teachers in providing WCF. This study compares the WCF produced by teachers and ChatGPT, examining their respective capabilities while identifying differences in their feedback practice. Findings reveal teacher provided WCF typically involves a consistent combination of direct correction and indirect feedback forms addressing both local and global issues, albeit with a degree of inaccuracy. ChatGPT-assisted WCF tends to be in the form of metalinguistic feedback and/or reformulation of the original text. However, GPT also frequently varies in its entire approach to WCF provision even when using the same prompt on the same text, while also providing grammatically accurate yet redundant WCF in certain cases. We discuss the implications of these findings for L2 writing practice.}
}
@article{ROSATI2025106207,
title = {The future of the movie industry in the wake of generative AI: A perspective under EU and UK copyright law},
journal = {Computer Law & Security Review},
volume = {59},
pages = {106207},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106207},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000793},
author = {Eleonora Rosati},
keywords = {Copyright, Artificial intelligence, Movie industry, LLM training, Authorship, Originality, Liability, Styles, Text and data mining, Exceptions},
abstract = {Executive summary
Like all sectors, the movie industry has been both affected by and exploring potential uses of generative Artificial Intelligence ('AI'). On the one hand, movie studios have detected and begun to add warnings against unlicensed third-party uses of their content, including for AI training,11W Cho, ‘Universal Pictures to Big Tech: We’ll sue if you steal our movies for AI’ (6 August 2025) The Hollywood Reporter, available at https://www.hollywoodreporter.com/business/business-news/universal-pictures-big-tech-well-sue-if-you-steal-movies-ai-1236337712/. and have taken enforcement initiatives through court action. On the other hand, the use of AI within and by the industry itself has been growing. Regarding the latter, some have emphasised the opportunities presented by the implementation of AI, including by advancing claims that AI tools can offer a `purer' form of expression. Others have instead warned against the potential displacement of industry workers, including workers employed in technical roles and younger and emerging actors. Against the background illustrated above, this study maps and critically evaluates relevant issues facing the development, deployment, and use of AI models from a movie industry perspective. The legal analysis is conducted having regard to EU and UK copyright law and is divided into three parts:•Input/AI training: By considering relevant legal restrictions applicable to the training of AI models on protected audiovisual content, the border between lawful unlicensed uses and restricted uses is drawn;•Protectability of AI-generated outputs: Turning to the output generation phase, the protectability of such outputs is considered next, by focusing in particular on the requirements of authorship and originality under EU and UK copyright law;•Legal risks and potential liability stemming from the use of third-party AI models for output generation: Still having regard to the output generation phase, relevant legal issues that might arise having regard to the use of AI models that `regurgitate' third-party training data at output generation are considered, alongside the question of style protection under copyright. The main conclusions are as follows:•Input/AI training: Insofar as model training on third-party protected content is concerned, there are no exceptions under EU/UK law that fully cover the entirety of these processes. As a result, lacking legislative reform, the establishment of a licensing framework appears unavoidable for such activities to be deemed lawful;•Protectability of AI-generated outputs: The deployment of AI across various phases of the creative process does not render the resulting content unprotectable, provided that human involvement and control remain significant throughout, with the result that AI is relied upon as a tool that aids – rather than replaces – the creativity of industry workers.•Legal risks and potential liability stemming from the use of third-party AI models for output generation: The use of AI models that generate infringing outputs, such as by regurgitating input data or merely imitating style, may trigger the application of exclusive rights under copyright and related rights. The resulting liability may vest with the user of such models, as well as the model developer/provider. The latter aspect means that terms that exclude any such liability may ultimately be found to be unenforceable against users and ineffective against rightholders.}
}